{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ACI PoV Lab Guide ACI\u81ea\u52a8\u5316\u4e0a\u7ebf ACI BareMetal\u670d\u52a1\u5668\u63a5\u5165 ACI VMWare VMM\u96c6\u6210 ACI L3out ACI\u9ad8\u6548\u8fd0\u7ef4 ACI\u81ea\u52a8\u5316 ACI Netflow ACI Red Hat VMM\u96c6\u6210 ACI K8s\u96c6\u6210 ACI CCP\u96c6\u6210","title":"Home"},{"location":"#aci-pov-lab-guide","text":"","title":"ACI PoV Lab Guide"},{"location":"#aci","text":"","title":"ACI\u81ea\u52a8\u5316\u4e0a\u7ebf"},{"location":"#aci-baremetal","text":"","title":"ACI BareMetal\u670d\u52a1\u5668\u63a5\u5165"},{"location":"#aci-vmware-vmm","text":"","title":"ACI VMWare VMM\u96c6\u6210"},{"location":"#aci-l3out","text":"","title":"ACI L3out"},{"location":"#aci_1","text":"","title":"ACI\u9ad8\u6548\u8fd0\u7ef4"},{"location":"#aci_2","text":"","title":"ACI\u81ea\u52a8\u5316"},{"location":"#aci-netflow","text":"","title":"ACI Netflow"},{"location":"#aci-red-hat-vmm","text":"","title":"ACI Red Hat VMM\u96c6\u6210"},{"location":"#aci-k8s","text":"","title":"ACI K8s\u96c6\u6210"},{"location":"#aci-ccp","text":"","title":"ACI CCP\u96c6\u6210"},{"location":"baremetal-index/","text":"ACI\u81ea\u52a8\u5316\u4e0a\u7ebf \u80cc\u666f\u77e5\u8bc6 \u672c\u6587\u5c06\u6f14\u793a\u5982\u4f55\u5b8c\u6210ACI\u9996\u6b21\u4e0a\u7ebf\u3002 ACI\u4e0a\u7ebf\u7684\u57fa\u672c\u6b65\u9aa4\u5982\u4e0b\uff1a \u7b2c\u4e00\u53f0APIC\u5f00\u673a \uff08CIMC, ACI initial setup script\uff09 \u53d1\u73b0\u5e76\u6ce8\u518c\u7b2c\u4e00\u53f0Leaf \u53d1\u73b0\u5e76\u6ce8\u518c\u7b2c\u4e00\u53f0Spine \u53d1\u73b0\u5e76\u6ce8\u518c\u5269\u4f59\u7684Leaf\u548cSpine \u6ce8\u518c\u5269\u4f59\u7684APIC \u81ea\u52a8\u5f00\u5c40\u5b8c\u6210 \u5b8c\u6210\u5176\u4ed6\u521d\u59cb\u5316\u914d\u7f6e\uff0c\u5982OOB, NTP\u7b49","title":"ACI BareMetal\u670d\u52a1\u5668\u63a5\u5165"},{"location":"baremetal-index/#aci","text":"","title":"ACI\u81ea\u52a8\u5316\u4e0a\u7ebf"},{"location":"baremetal-index/#_1","text":"\u672c\u6587\u5c06\u6f14\u793a\u5982\u4f55\u5b8c\u6210ACI\u9996\u6b21\u4e0a\u7ebf\u3002 ACI\u4e0a\u7ebf\u7684\u57fa\u672c\u6b65\u9aa4\u5982\u4e0b\uff1a \u7b2c\u4e00\u53f0APIC\u5f00\u673a \uff08CIMC, ACI initial setup script\uff09 \u53d1\u73b0\u5e76\u6ce8\u518c\u7b2c\u4e00\u53f0Leaf \u53d1\u73b0\u5e76\u6ce8\u518c\u7b2c\u4e00\u53f0Spine \u53d1\u73b0\u5e76\u6ce8\u518c\u5269\u4f59\u7684Leaf\u548cSpine \u6ce8\u518c\u5269\u4f59\u7684APIC \u81ea\u52a8\u5f00\u5c40\u5b8c\u6210 \u5b8c\u6210\u5176\u4ed6\u521d\u59cb\u5316\u914d\u7f6e\uff0c\u5982OOB, NTP\u7b49","title":"\u80cc\u666f\u77e5\u8bc6"},{"location":"ccp/ccp-calico-setup/","text":"\u521b\u5efaCCP Calico Cluster CCP\u754c\u9762, \u70b9\u51fb\u65b0\u5efaCluster 01-\u8f93\u5165\u57fa\u7840\u4fe1\u606f Input basic Information 02-\u8f93\u5165Provider\u4fe1\u606f Input Provider information 03-\u8f93\u5165Node\u4fe1\u606f Input Node information 04-\u8f93\u5165ASW IAM\u4fe1\u606f Input ASW IAM information 05-\u5b8c\u6210Cluster\u7684\u521b\u5efa finish the Cluster creation","title":"CCP Calico Cluster \u8bbe\u7f6e"},{"location":"ccp/ccp-calico-setup/#ccp-calico-cluster","text":"CCP\u754c\u9762, \u70b9\u51fb\u65b0\u5efaCluster","title":"\u521b\u5efaCCP Calico Cluster"},{"location":"ccp/ccp-calico-setup/#01-","text":"Input basic Information","title":"01-\u8f93\u5165\u57fa\u7840\u4fe1\u606f"},{"location":"ccp/ccp-calico-setup/#02-provider","text":"Input Provider information","title":"02-\u8f93\u5165Provider\u4fe1\u606f"},{"location":"ccp/ccp-calico-setup/#03-node","text":"Input Node information","title":"03-\u8f93\u5165Node\u4fe1\u606f"},{"location":"ccp/ccp-calico-setup/#04-asw-iam","text":"Input ASW IAM information","title":"04-\u8f93\u5165ASW IAM\u4fe1\u606f"},{"location":"ccp/ccp-calico-setup/#05-cluster","text":"finish the Cluster creation","title":"05-\u5b8c\u6210Cluster\u7684\u521b\u5efa"},{"location":"ccp/ccp-guestbook/","text":"\u90e8\u7f72Guestbook\u5bb9\u5668\u5e94\u7528 Guestbook\u90e8\u7f72\u53c2\u8003\u94fe\u63a5\uff1a https://kubernetes.io/zh/docs/tutorials/stateless-application/guestbook/ 1. SSH\u767b\u5f55Group Master Node SSH login to group master node 2. \u90e8\u7f72Redis\u4e3bDeployment deploy redis main deployment kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-deployment.yaml 3. \u521b\u5efaRedis \u4e3b\u8282\u70b9service create Redis master node service kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-service.yaml 3. \u521b\u5efaRedis \u4ece\u8282\u70b9deployment create Redis worker node deployment kubectl apply -f https://k8s.io/examples/application/guestbook/redis-slave-deployment.yaml 4. \u521b\u5efaRedis \u4ece\u8282\u70b9service create Redis worker node service kubectl apply -f https://k8s.io/examples/application/guestbook/redis-slave-service.yaml 5. \u521b\u5efa\u7559\u8a00\u677f\u524d\u7aef Deployment create guestbook front-end deployment kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-deployment.yaml 6. \u521b\u5efa\u7559\u8a00\u677f\u524d\u7aef service create guestbook front-end service \u4e0b\u8f7dfrontend service yaml file wget https://k8s.io/examples/application/guestbook/frontend-service.yaml \u7f16\u8f91yaml\u6587\u4ef6\uff0c\u5c06type: NodePort \u6539\u4e3atype: LoadBalancer apiVersion: v1 kind: Service metadata: name: frontend labels: app: guestbook tier: frontend spec: # comment or delete the following line if you want to use a LoadBalancer type: LoadBalancer # if your cluster supports it, uncomment the following to automatically create # an external load-balanced IP for the frontend service. # type: LoadBalancer ports: - port: 80 selector: app: guestbook tier: frontend \u5e94\u7528\u65b0\u7684yaml\u6587\u4ef6 kubectl apply -f frontend-service.yaml","title":"\u90e8\u7f72\u5bb9\u5668\u5e94\u7528"},{"location":"ccp/ccp-guestbook/#guestbook","text":"Guestbook\u90e8\u7f72\u53c2\u8003\u94fe\u63a5\uff1a https://kubernetes.io/zh/docs/tutorials/stateless-application/guestbook/","title":"\u90e8\u7f72Guestbook\u5bb9\u5668\u5e94\u7528"},{"location":"ccp/ccp-guestbook/#1-sshgroup-master-node","text":"SSH login to group master node","title":"1. SSH\u767b\u5f55Group Master Node"},{"location":"ccp/ccp-guestbook/#2-redisdeployment","text":"deploy redis main deployment kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-deployment.yaml","title":"2. \u90e8\u7f72Redis\u4e3bDeployment"},{"location":"ccp/ccp-guestbook/#3-redis-service","text":"create Redis master node service kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-service.yaml","title":"3. \u521b\u5efaRedis \u4e3b\u8282\u70b9service"},{"location":"ccp/ccp-guestbook/#3-redis-deployment","text":"create Redis worker node deployment kubectl apply -f https://k8s.io/examples/application/guestbook/redis-slave-deployment.yaml","title":"3. \u521b\u5efaRedis \u4ece\u8282\u70b9deployment"},{"location":"ccp/ccp-guestbook/#4-redis-service","text":"create Redis worker node service kubectl apply -f https://k8s.io/examples/application/guestbook/redis-slave-service.yaml","title":"4. \u521b\u5efaRedis \u4ece\u8282\u70b9service"},{"location":"ccp/ccp-guestbook/#5-deployment","text":"create guestbook front-end deployment kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-deployment.yaml","title":"5. \u521b\u5efa\u7559\u8a00\u677f\u524d\u7aef Deployment"},{"location":"ccp/ccp-guestbook/#6-service","text":"create guestbook front-end service \u4e0b\u8f7dfrontend service yaml file wget https://k8s.io/examples/application/guestbook/frontend-service.yaml \u7f16\u8f91yaml\u6587\u4ef6\uff0c\u5c06type: NodePort \u6539\u4e3atype: LoadBalancer apiVersion: v1 kind: Service metadata: name: frontend labels: app: guestbook tier: frontend spec: # comment or delete the following line if you want to use a LoadBalancer type: LoadBalancer # if your cluster supports it, uncomment the following to automatically create # an external load-balanced IP for the frontend service. # type: LoadBalancer ports: - port: 80 selector: app: guestbook tier: frontend \u5e94\u7528\u65b0\u7684yaml\u6587\u4ef6 kubectl apply -f frontend-service.yaml","title":"6. \u521b\u5efa\u7559\u8a00\u677f\u524d\u7aef service"},{"location":"ccp/ccp-index/","text":"ACI CCP \u96c6\u6210 \u80cc\u666f\u4fe1\u606f Cisco Container Platform supports multiple Kubernetes CNI plugins such as: ACI is the recommended plugin for use with an ACI fabric. It is optimized for use with an ACI fabric. ACI is fully supported by Cisco. Calico is recommended when an ACI fabric is not used. Contiv is a user space switch that is optimized for high performance and scale","title":"\u80cc\u666f\u4fe1\u606f"},{"location":"ccp/ccp-index/#aci-ccp","text":"","title":"ACI CCP \u96c6\u6210"},{"location":"ccp/ccp-index/#_1","text":"Cisco Container Platform supports multiple Kubernetes CNI plugins such as: ACI is the recommended plugin for use with an ACI fabric. It is optimized for use with an ACI fabric. ACI is fully supported by Cisco. Calico is recommended when an ACI fabric is not used. Contiv is a user space switch that is optimized for high performance and scale","title":"\u80cc\u666f\u4fe1\u606f"},{"location":"ccp/ccp-installation/","text":"CCP\u5b89\u88c5 Part 1 - VM\u5b89\u88c5 \u5b89\u88c5 CCP VM #\u4e0b\u8f7d\u6700\u65b0\u7248\u672c\u7684CCP\u5b89\u88c5\u4ecb\u8d28, \u89e3\u538b\u7f29\u4e3aOVA\u6587\u4ef6 CCO CCP Download Link installer VM - kcp VM e.g. kcp-vm-6.0.0.ova.tar.gz node base VM - ccp tenant VM e.g. ccp-tenant-image-1.16.3-ubuntu18-6.0.0.ova.tar.gz #\u5b89\u88c5installer VM \u5728vCenter\u4e2d\u5bfc\u5165KCP VM \u5b58\u50a8\u8bbe\u7f6e \u7f51\u7edc\u8bbe\u7f6e\uff0c port-group\u5e94\u4e0e\u4e0a\u9762\u7684ASAv DHCP interface\u7f51\u6bb5\u6253\u901a\uff0c\u80fd\u591f\u53d6\u5f97DHCP\u5730\u5740 \u5728\u5ba2\u6237\u7aef\u751f\u6210SSH public key MENTAN-M-J0CJ:~ mentan$ ssh-keygen -t ecdsa Generating public/private ecdsa key pair. Enter file in which to save the key (/Users/mentan/.ssh/id_ecdsa): /Users/mentan/.ssh/id_ecdsa already exists. Overwrite (y/n)? y Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /Users/mentan/.ssh/id_ecdsa. Your public key has been saved in /Users/mentan/.ssh/id_ecdsa.pub. The key fingerprint is: SHA256:BDFn7c6M87rtD7QaO6yE1WsIUUUEUPtb2Qn4nWwzQKc mentan@MENTAN-M-J0CJ The key's randomart image is: +---[ECDSA 256]---+ | .oB**.. . | | . * o.o | | . . o.E | | . + ..B o | | . . S== X | | + .o=+o o | | . o.=oo | | . .o=.. | | ..==o.. | +----[SHA256]-----+ MENTAN-M-J0CJ:~ mentan$ cd .ssh MENTAN-M-J0CJ:.ssh mentan$ ls -lth total 40 -rw-r--r-- 1 mentan staff 182B Apr 9 21:47 id_ecdsa.pub -rw------- 1 mentan staff 513B Apr 9 21:47 id_ecdsa -rw-r--r-- 1 mentan staff 3.1K Apr 8 00:31 known_hosts -rw-------@ 1 mentan staff 85B Jan 7 21:10 ccp_ed25519.pub -r-xr-xr-x@ 1 mentan staff 387B Jan 7 21:10 ccp_ed25519 MENTAN-M-J0CJ:.ssh mentan$ more id_ecdsa.pub ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBCKWJC3eExw2QrFIzy/XmKtDk92B2aVu+Or0ZeqjkI57p3y/clJz9za+dMwa11hAJ6YoNMD/G2uVxFCJBFhErSw= mentan@MENTAN-M-J0CJ \u5b9a\u5236\u5316\u754c\u9762\uff0c\u8f93\u5165SSH Public key \u5b8c\u6210\u5b89\u88c5 VM\u90e8\u7f72\u5b8c\u6210\u540e\uff0c\u521b\u5efasnapshot \u5728vCenter\u4e2d\u5bfc\u5165node tenant VM (base VM). tenant VM\u5728\u6574\u4e2a\u5b89\u88c5\u8fc7\u7a0b\u4e2d\u65e0\u9700\u5f00\u673a\uff0c\u53ea\u662f\u4f5c\u4e3aK8s node\u514b\u9686\u6a21\u677f\u7528\u9014\u3002 \u5b58\u50a8\u8bbe\u7f6e \u7f51\u7edc\u8bbe\u7f6e \u5b9a\u5236\u5316\u754c\u9762\uff0c\u8f93\u5165\u5bc6\u7801 \u5b8c\u6210\u5b89\u88c5 CCP\u5b89\u88c5 Part 2 - KCP VM \u8bbe\u7f6e \u5c06KCP VM\u5f00\u673a\uff0c VM\u5c06\u4eceDHCP\u670d\u52a1\u5668\u5f97\u5230IP\u5730\u5740\u5e76\u7ed9\u51fa\u63a7\u5236\u53f0\u94fe\u63a5 \u901a\u8fc7\u7ed9\u51fa\u7684\u94fe\u63a5\uff0c\u8bbf\u95eeCCP\u7ba1\u7406\u63a7\u5236\u53f0 \u70b9\u51fb Install\uff0c\u5f00\u59cb\u5b89\u88c5 01-\u8f93\u5165vcenter\u767b\u5f55\u4fe1\u606f 02-\u8f93\u5165vcenter\u5b89\u7f6e\u4fe1\u606f 03-\u8f93\u5165CCP Cluster\u4fe1\u606f (ACI-CNI) 03-\u8f93\u5165CCP Cluster\u4fe1\u606f (Calico CNI) Input CCP Cluster information (Calico CNI) * 04-\u8f93\u5165\u7f51\u7edc\u4fe1\u606f Input network information * 05-\u8f93\u5165\u8ba4\u8bc1\u4fe1\u606f\uff0c\u91c7\u7528\u672c\u5730\u8ba4\u8bc1 * 06-\u8f93\u5165\u63a7\u5236\u5e73\u9762\u4fe1\u606f\uff0c\u5b8c\u6210\u8bbe\u7f6e * \u5b89\u88c5\u5b8c\u6210\u540e\u7ed9\u51faCCP\u5165\u53e3\u5730\u5740 CCP Control-plane master/worker\u8bbe\u7f6e \u5f53Installer\u5b8c\u6210\u540e\uff0c\u4f1a\u751f\u6210\u4e0b\u5217Control-plane VM\uff1a 1* Master 3* Worker \u68c0\u67e5VM IP \u5730\u5740 \u5bf9 \u6240\u6709 master/worker, ssh\u767b\u5f55\u5e76\u6dfb\u52a0\u5230group-master/worker\u7684\u8def\u7531","title":"CCP\u5b89\u88c5"},{"location":"ccp/ccp-installation/#ccp-part-1-vm","text":"","title":"CCP\u5b89\u88c5 Part 1 - VM\u5b89\u88c5"},{"location":"ccp/ccp-installation/#ccp-vm","text":"","title":"\u5b89\u88c5 CCP VM"},{"location":"ccp/ccp-installation/#ccp-ova","text":"CCO CCP Download Link installer VM - kcp VM e.g. kcp-vm-6.0.0.ova.tar.gz node base VM - ccp tenant VM e.g. ccp-tenant-image-1.16.3-ubuntu18-6.0.0.ova.tar.gz","title":"#\u4e0b\u8f7d\u6700\u65b0\u7248\u672c\u7684CCP\u5b89\u88c5\u4ecb\u8d28, \u89e3\u538b\u7f29\u4e3aOVA\u6587\u4ef6"},{"location":"ccp/ccp-installation/#installer-vm","text":"\u5728vCenter\u4e2d\u5bfc\u5165KCP VM \u5b58\u50a8\u8bbe\u7f6e \u7f51\u7edc\u8bbe\u7f6e\uff0c port-group\u5e94\u4e0e\u4e0a\u9762\u7684ASAv DHCP interface\u7f51\u6bb5\u6253\u901a\uff0c\u80fd\u591f\u53d6\u5f97DHCP\u5730\u5740 \u5728\u5ba2\u6237\u7aef\u751f\u6210SSH public key MENTAN-M-J0CJ:~ mentan$ ssh-keygen -t ecdsa Generating public/private ecdsa key pair. Enter file in which to save the key (/Users/mentan/.ssh/id_ecdsa): /Users/mentan/.ssh/id_ecdsa already exists. Overwrite (y/n)? y Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /Users/mentan/.ssh/id_ecdsa. Your public key has been saved in /Users/mentan/.ssh/id_ecdsa.pub. The key fingerprint is: SHA256:BDFn7c6M87rtD7QaO6yE1WsIUUUEUPtb2Qn4nWwzQKc mentan@MENTAN-M-J0CJ The key's randomart image is: +---[ECDSA 256]---+ | .oB**.. . | | . * o.o | | . . o.E | | . + ..B o | | . . S== X | | + .o=+o o | | . o.=oo | | . .o=.. | | ..==o.. | +----[SHA256]-----+ MENTAN-M-J0CJ:~ mentan$ cd .ssh MENTAN-M-J0CJ:.ssh mentan$ ls -lth total 40 -rw-r--r-- 1 mentan staff 182B Apr 9 21:47 id_ecdsa.pub -rw------- 1 mentan staff 513B Apr 9 21:47 id_ecdsa -rw-r--r-- 1 mentan staff 3.1K Apr 8 00:31 known_hosts -rw-------@ 1 mentan staff 85B Jan 7 21:10 ccp_ed25519.pub -r-xr-xr-x@ 1 mentan staff 387B Jan 7 21:10 ccp_ed25519 MENTAN-M-J0CJ:.ssh mentan$ more id_ecdsa.pub ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBCKWJC3eExw2QrFIzy/XmKtDk92B2aVu+Or0ZeqjkI57p3y/clJz9za+dMwa11hAJ6YoNMD/G2uVxFCJBFhErSw= mentan@MENTAN-M-J0CJ \u5b9a\u5236\u5316\u754c\u9762\uff0c\u8f93\u5165SSH Public key \u5b8c\u6210\u5b89\u88c5 VM\u90e8\u7f72\u5b8c\u6210\u540e\uff0c\u521b\u5efasnapshot \u5728vCenter\u4e2d\u5bfc\u5165node tenant VM (base VM). tenant VM\u5728\u6574\u4e2a\u5b89\u88c5\u8fc7\u7a0b\u4e2d\u65e0\u9700\u5f00\u673a\uff0c\u53ea\u662f\u4f5c\u4e3aK8s node\u514b\u9686\u6a21\u677f\u7528\u9014\u3002 \u5b58\u50a8\u8bbe\u7f6e \u7f51\u7edc\u8bbe\u7f6e \u5b9a\u5236\u5316\u754c\u9762\uff0c\u8f93\u5165\u5bc6\u7801 \u5b8c\u6210\u5b89\u88c5","title":"#\u5b89\u88c5installer VM"},{"location":"ccp/ccp-installation/#ccp-part-2-kcp-vm","text":"\u5c06KCP VM\u5f00\u673a\uff0c VM\u5c06\u4eceDHCP\u670d\u52a1\u5668\u5f97\u5230IP\u5730\u5740\u5e76\u7ed9\u51fa\u63a7\u5236\u53f0\u94fe\u63a5 \u901a\u8fc7\u7ed9\u51fa\u7684\u94fe\u63a5\uff0c\u8bbf\u95eeCCP\u7ba1\u7406\u63a7\u5236\u53f0 \u70b9\u51fb Install\uff0c\u5f00\u59cb\u5b89\u88c5 01-\u8f93\u5165vcenter\u767b\u5f55\u4fe1\u606f 02-\u8f93\u5165vcenter\u5b89\u7f6e\u4fe1\u606f 03-\u8f93\u5165CCP Cluster\u4fe1\u606f (ACI-CNI) 03-\u8f93\u5165CCP Cluster\u4fe1\u606f (Calico CNI) Input CCP Cluster information (Calico CNI) * 04-\u8f93\u5165\u7f51\u7edc\u4fe1\u606f Input network information * 05-\u8f93\u5165\u8ba4\u8bc1\u4fe1\u606f\uff0c\u91c7\u7528\u672c\u5730\u8ba4\u8bc1 * 06-\u8f93\u5165\u63a7\u5236\u5e73\u9762\u4fe1\u606f\uff0c\u5b8c\u6210\u8bbe\u7f6e * \u5b89\u88c5\u5b8c\u6210\u540e\u7ed9\u51faCCP\u5165\u53e3\u5730\u5740","title":"CCP\u5b89\u88c5 Part 2 - KCP VM \u8bbe\u7f6e"},{"location":"ccp/ccp-installation/#ccp-control-plane-masterworker","text":"\u5f53Installer\u5b8c\u6210\u540e\uff0c\u4f1a\u751f\u6210\u4e0b\u5217Control-plane VM\uff1a 1* Master 3* Worker \u68c0\u67e5VM IP \u5730\u5740 \u5bf9 \u6240\u6709 master/worker, ssh\u767b\u5f55\u5e76\u6dfb\u52a0\u5230group-master/worker\u7684\u8def\u7531","title":"CCP Control-plane master/worker\u8bbe\u7f6e"},{"location":"ccp/ccp-prerequisite/","text":"CC\u5b89\u88c5\u524d\u51c6\u5907 \u5730\u5740/VLAN NTP: 10.66.141.50, 10.66.141.51 DNS: 64.104.76.247, 64.204.200.248 CCP management IP: 10.75.53.0/28 (10.75.53.2 - 16) CIDR For Controller Kubernetes Pod network(Installer): 10.50.0.0/16 subnet for Pods(ACI-CNI): 10.51.0.1/16 subnet for service(ACI-CNI): 10.52.0.1/16 node vlan start ID: 550 (HX FI trunk allow) node vlan end ID: 560 (HX FI trunk allow) DHCP Server \u4f7f\u7528ASAv\u63d0\u4f9bDHCP\u529f\u80fd\uff0cASAv\u914d\u7f6e\u5982\u4e0b\uff1a interface GigabitEthernet0/0 nameif inside security-level 100 ip address 10.75.53.172 255.255.255.0 dhcpd option 3 ip 10.75.53.1 ! dhcpd address 10.75.53.71-10.75.53.72 inside dhcpd enable inside ACI\u73af\u5883 CCP installer VM \u8bbe\u7f6e\u4e2d\u4f7f\u7528\u7684AAEP/Tenant/VRF\u53c2\u6570 AAEP: mentan-ccp6 Tenant: mentan-CCP6 VRF: common/default \u914d\u7f6eL3out in common tenant\u7528\u4e8e\u8bbf\u95eeInternet\uff0c\u4e0b\u8f7d\u76f8\u5173\u6587\u4ef6 \u6309\u7167\u6807\u51c6\u65b9\u6cd5\u5728common tenant\u4e2d\u914d\u7f6eL3out\uff08\u8fc7\u7a0b\u7701\u7565\uff09 \u5728L3out subnet\uff0c\u5f00\u542fleaking","title":"\u96c6\u6210\u524d\u51c6\u5907"},{"location":"ccp/ccp-prerequisite/#cc","text":"","title":"CC\u5b89\u88c5\u524d\u51c6\u5907"},{"location":"ccp/ccp-prerequisite/#vlan","text":"NTP: 10.66.141.50, 10.66.141.51 DNS: 64.104.76.247, 64.204.200.248 CCP management IP: 10.75.53.0/28 (10.75.53.2 - 16) CIDR For Controller Kubernetes Pod network(Installer): 10.50.0.0/16 subnet for Pods(ACI-CNI): 10.51.0.1/16 subnet for service(ACI-CNI): 10.52.0.1/16 node vlan start ID: 550 (HX FI trunk allow) node vlan end ID: 560 (HX FI trunk allow)","title":"\u5730\u5740/VLAN"},{"location":"ccp/ccp-prerequisite/#dhcp-server","text":"\u4f7f\u7528ASAv\u63d0\u4f9bDHCP\u529f\u80fd\uff0cASAv\u914d\u7f6e\u5982\u4e0b\uff1a interface GigabitEthernet0/0 nameif inside security-level 100 ip address 10.75.53.172 255.255.255.0 dhcpd option 3 ip 10.75.53.1 ! dhcpd address 10.75.53.71-10.75.53.72 inside dhcpd enable inside","title":"DHCP Server"},{"location":"ccp/ccp-prerequisite/#aci","text":"CCP installer VM \u8bbe\u7f6e\u4e2d\u4f7f\u7528\u7684AAEP/Tenant/VRF\u53c2\u6570 AAEP: mentan-ccp6 Tenant: mentan-CCP6 VRF: common/default \u914d\u7f6eL3out in common tenant\u7528\u4e8e\u8bbf\u95eeInternet\uff0c\u4e0b\u8f7d\u76f8\u5173\u6587\u4ef6 \u6309\u7167\u6807\u51c6\u65b9\u6cd5\u5728common tenant\u4e2d\u914d\u7f6eL3out\uff08\u8fc7\u7a0b\u7701\u7565\uff09 \u5728L3out subnet\uff0c\u5f00\u542fleaking","title":"ACI\u73af\u5883"},{"location":"ccp/ccp-troubleshooting/","text":"CCP \u6392\u969c Issue 1\uff1a coredns pod stuck in pending \u6545\u969c\u73b0\u8c61\uff1a coredns pod\u5904\u4e8epending\u72b6\u6001\u5bfc\u81f4master node\u90e8\u7f72\u5931\u8d25 Cluster\u4e2d\u7684master node\u90e8\u7f72\u5931\u8d25 \u6392\u969c\u8fc7\u7a0b Note: CCP\u90e8\u7f72\u7684node\u662fminimal\u7684ubuntu\uff0c\u4e0d\u5305\u542bping\u7b49\u547d\u4ee4\uff0c\u53ef\u4ee5\u4f7f\u7528\u4e0b\u9762\u7684\u65b9\u6cd5\u52a0\u5165ping\u5e94\u7528 apt-get update apt-get install iputils-ping \u68c0\u67e5coredns log\uff0c\u6ca1\u6709\u8f93\u51fa ccpuser@mentan-ccp6-master-gro-6e6734d30b:~$ ccpuser@mentan-ccp6-master-gro-6e6734d30b:~$ kubectl logs -f coredns-65f5bff896-wz9hb -n kube-system ccpuser@mentan-ccp6-master-gro-6e6734d30b:~$ \u67e5\u770bcoredns pod\u8be6\u7ec6\u4fe1\u606f ccpuser@mentan-ccp6-master-gro-6e6734d30b:~$ kubectl describe pod coredns-65f5bff896-wz9hb -n kube-system Name: coredns-65f5bff896-wz9hb Namespace: kube-system Priority: 2000000000 Priority Class Name: system-cluster-critical Node: none Labels: k8s-app=kube-dns pod-template-hash=65f5bff896 Annotations: none Status: Pending IP: IPs: none Controlled By: ReplicaSet/coredns-65f5bff896 Containers: coredns: Image: registry.ci.ciscolabs.com/cpsg_ccp/k8s.gcr.io/coredns:1.6.2 Ports: 53/UDP, 53/TCP, 9153/TCP Host Ports: 0/UDP, 0/TCP, 0/TCP Args: -conf /etc/coredns/Corefile Limits: memory: 170Mi Requests: cpu: 100m memory: 70Mi Liveness: http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5 Readiness: http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3 Environment: none Mounts: /etc/coredns from config-volume (ro) /var/run/secrets/kubernetes.io/serviceaccount from coredns-token-z9hqz (ro) Conditions: Type Status PodScheduled False Volumes: config-volume: Type: ConfigMap (a volume populated by a ConfigMap) Name: coredns Optional: false coredns-token-z9hqz: Type: Secret (a volume populated by a Secret) SecretName: coredns-token-z9hqz Optional: false QoS Class: Burstable Node-Selectors: beta.kubernetes.io/os=linux Tolerations: CriticalAddonsOnly node-role.kubernetes.io/master:NoSchedule node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 89s (x297 over 7h22m) default-scheduler 0/1 nodes are available: 1 node(s) had taints that the pod didn't tolerate. ccpuser@mentan-ccp6-master-gro-6e6734d30b:~$ \u68c0\u67e5coredns ConfigMap kubectl edit cm coredns -n kube-system # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 data: Corefile: \".:53 {\\n errors\\n health\\n ready\\n kubernetes cluster.local in-addr.arpa ip6.arpa {\\n pods insecure\\n fallthrough in-addr.arpa ip6.arpa\\n ttl 30\\n }\\n prometheus :9153\\n forward . /etc/resolv.conf\\n \\ cache 30\\n \\n reload\\n loadbalance\\n}\\n\" kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"v1\",\"data\":{\"Corefile\":\".:53 {\\n errors\\n health\\n ready\\n kubernetes cluster.local in-addr.arpa ip6.arpa {\\n pods insecure\\n fallthrough in-addr.arpa ip6.arpa\\n ttl 30\\n }\\n prometheus :9153\\n forward . /etc/resolv.conf\\n cache 30\\n \\n reload\\n loadbalance\\n}\\n\"},\"kind\":\"ConfigMap\",\"metadata\":{\"annotations\":{},\"creationTimestamp\":\"2020-04-16T16:58:00Z\",\"name\":\"coredns\",\"namespace\":\"kube-system\",\"resourceVersion\":\"175\",\"selfLink\":\"/api/v1/namespaces/kube-system/configmaps/coredns\",\"uid\":\"3ed0b0e3-640a-4dda-98ff-8fc897e159c4\"}} creationTimestamp: \"2020-04-16T16:58:00Z\" name: coredns namespace: kube-system resourceVersion: \"224\" selfLink: /api/v1/namespaces/kube-system/configmaps/coredns uid: 3ed0b0e3-640a-4dda-98ff-8fc897e159c4 \u68c0\u67e5resolv.conf\u914d\u7f6e ccpuser@mentan-ccp6-master-gro-6e6734d30b:~$ more /etc/resolv.conf # Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8) # DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN # 127.0.0.53 is the systemd-resolved stub resolver. # run \"systemd-resolve --status\" to see details about the actual nameservers. nameserver 64.104.123.245 \u901a\u8fc7nslookup\u68c0\u67e5\u4e3b\u673aDNS\uff0c\u4e00\u5207\u6b63\u5e38\u3002 \u68c0\u67e5CCP\u5b89\u88c5\u65e5\u5fd7, \u6709\u5f88\u591a\"network plugin is not ready\"\u7684\u9519\u8bef more /var/log/syslog ... Apr 20 03:16:19 mentan-ccp6-master-gro-fedf9b35d0 kubelet[3358]: W0420 03:16:19.212243 3358 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d Apr 20 03:16:23 mentan-ccp6-master-gro-fedf9b35d0 kubelet[3358]: E0420 03:16:23.267608 3358 kubelet.go:2187] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized Apr 20 03:16:24 mentan-ccp6-master-gro-fedf9b35d0 kubelet[3358]: W0420 03:16:24.212638 3358 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d Apr 20 03:16:24 mentan-ccp6-master-gro-fedf9b35d0 systemd[1]: Started Session 21 of user ccpuser. Apr 20 03:16:28 mentan-ccp6-master-gro-fedf9b35d0 kubelet[3358]: E0420 03:16:28.269207 3358 kubelet.go:2187] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized Apr 20 03:16:29 mentan-ccp6-master-gro-fedf9b35d0 kubelet[3358]: W0420 03:16:29.213033 3358 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d \u68c0\u67e5HX MTU, \u5df2\u7ecf\u88ab\u8bbe\u7f6e\u4e3a9216 ccpuser@mentan-ccp6-master-gro-fedf9b35d0:~$ kubectl describe node Name: mentan-ccp6-master-gro-fedf9b35d0 Roles: master Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/arch=amd64 kubernetes.io/hostname=mentan-ccp6-master-gro-fedf9b35d0 kubernetes.io/os=linux node-role.kubernetes.io/master= Annotations: kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock node.alpha.kubernetes.io/ttl: 0 volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Mon, 20 Apr 2020 02:05:54 +0000 Taints: node-role.kubernetes.io/master:NoSchedule node.kubernetes.io/not-ready:NoSchedule Unschedulable: false Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Mon, 20 Apr 2020 07:08:03 +0000 Mon, 20 Apr 2020 02:05:50 +0000 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Mon, 20 Apr 2020 07:08:03 +0000 Mon, 20 Apr 2020 02:05:50 +0000 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Mon, 20 Apr 2020 07:08:03 +0000 Mon, 20 Apr 2020 02:05:50 +0000 KubeletHasSufficientPID kubelet has sufficient PID available Ready False Mon, 20 Apr 2020 07:08:03 +0000 Mon, 20 Apr 2020 02:05:50 +0000 KubeletNotReady runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized Addresses: ExternalIP: 192.168.100.10 InternalIP: 192.168.100.10 Hostname: mentan-ccp6-master-gro-fedf9b35d0 Capacity: cpu: 2 ephemeral-storage: 40470732Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 16426480Ki pods: 110 Allocatable: cpu: 2 ephemeral-storage: 37297826550 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 16324080Ki pods: 110 System Info: Machine ID: 93c34496f87f4a39b4b6d51766ee71a7 System UUID: 4203BCE9-816E-EB82-BDF0-97599E3F9731 Boot ID: 2479c7d3-3fe9-4112-b5f6-d2a4b1f7e54a Kernel Version: 4.15.0-64-generic OS Image: Ubuntu 18.04.3 LTS Operating System: linux Architecture: amd64 Container Runtime Version: docker://18.9.9 Kubelet Version: v1.16.3 Kube-Proxy Version: v1.16.3 PodCIDR: 10.51.0.0/24 PodCIDRs: 10.51.0.0/24 ProviderID: vsphere://4203bce9-816e-eb82-bdf0-97599e3f9731 Non-terminated Pods: (6 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- ccp ccp-vip-manager-mentan-ccp6-master-gro-fedf9b35d0 0 (0%) 0 (0%) 0 (0%) 0 (0%) 5h1m kube-system etcd-mentan-ccp6-master-gro-fedf9b35d0 0 (0%) 0 (0%) 0 (0%) 0 (0%) 5h1m kube-system kube-apiserver-mentan-ccp6-master-gro-fedf9b35d0 250m (12%) 0 (0%) 0 (0%) 0 (0%) 5h1m kube-system kube-controller-manager-mentan-ccp6-master-gro-fedf9b35d0 200m (10%) 0 (0%) 0 (0%) 0 (0%) 5h1m kube-system kube-proxy-xpdff 0 (0%) 0 (0%) 0 (0%) 0 (0%) 5h2m kube-system kube-scheduler-mentan-ccp6-master-gro-fedf9b35d0 100m (5%) 0 (0%) 0 (0%) 0 (0%) 5h1m Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 550m (27%) 0 (0%) memory 0 (0%) 0 (0%) ephemeral-storage 0 (0%) 0 (0%) Events: none \u4ecegroup master \u7528Jumbo packet ping control-plane master, \u53d1\u73b0\u4e22\u5305 ccpuser@mentan-ccp6-master-gro-8a05272381:~$ tracepath -n 10.75.53.3 1?: [LOCALHOST] pmtu 1600 1: 172.16.100.1 0.576ms asymm 2 1: 172.16.100.1 0.416ms asymm 2 2: 192.168.254.2 0.666ms 3: no reply 4: no reply 5: no reply 6: no reply 7: no reply 8: no reply 9: no reply 10: no reply 11: no reply 12: no reply 13: no reply 14: no reply 15: no reply 16: no reply 17: no reply 18: no reply 19: no reply 20: no reply 21: no reply 22: no reply 23: no reply 24: no reply 25: no reply 26: no reply 27: no reply 28: no reply 29: no reply 30: no reply Too many hops: pmtu 1600 Resume: pmtu 1600 ccpuser@mentan-ccp6-master-gro-8a05272381:~$ \u7ecf\u8fc7\u68b3\u7406lab\u62d3\u6251\uff0c\u53d1\u73b0\u53ef\u80fd\u5b58\u5728MTU\u7684\u95ee\u9898\u5728\u4e0a\u884c\u7684OOB 4948 switch \u6839\u636e\u4e0b\u9762\u76844500 Jumbo frame\u6392\u9519\u6587\u6863\uff0c\u5c06OOB 4948 switch\u7684MTU\u8c03\u6574\u4e3a9000 https://www.cisco.com/c/en/us/support/docs/switches/catalyst-4000-series-switches/29805-175.html OOB 4948 MTU\u72b6\u6001\u5982\u4e0b ACI_Fabric2_OOB#show vlan mtu VLAN SVI_MTU MinMTU(port) MaxMTU(port) MTU_Mismatch ---- ------------- ---------------- --------------- ------------ 1 9000 9000 9000 No 1002 - 9000 9000 No 1003 - 9000 9000 No 1004 - 9000 9000 No 1005 - 9000 9000 No ACI_Fabric2_OOB#show interfaces mtu Port Name MTU Gi1/1 9000 Gi1/2 9000 Gi1/3 9000 Gi1/4 9000 Gi1/5 9000 Gi1/6 9000 Gi1/7 9000 Gi1/8 9000 Gi1/9 9000 Gi1/10 9000 Gi1/11 9000 Gi1/12 9000 Gi1/13 9000 Gi1/14 9000 Gi1/15 9000 Gi1/16 9000 Gi1/17 9000 Gi1/18 9000 Gi1/19 9000 Gi1/20 9000 Gi1/21 9000 Gi1/22 9000 Gi1/23 9000 Gi1/24 9000 Gi1/25 9000 Gi1/26 9000 Gi1/27 9000 Gi1/28 9000 Gi1/29 9000 Gi1/30 9000 Gi1/31 9000 Gi1/32 9000 Gi1/33 9000 Gi1/34 9000 Gi1/35 9000 Gi1/36 9000 Gi1/37 9000 Gi1/38 9000 Gi1/39 9000 Gi1/40 9000 Gi1/41 9000 Gi1/42 9000 Gi1/43 9000 Gi1/44 9000 Gi1/45 9000 Gi1/46 9000 Gi1/47 9000 Gi1/48 9000 Te1/49 9000 Te1/50 9000 ACI_Fabric2_OOB# \u7ecf\u8fc7\u4e0a\u8ff0\u8c03\u6574\u540e\uff0c\u53d1\u73b0MTU\u95ee\u9898\u89e3\u51b3 ccpuser@mentan-ccp6-master-gro-8a05272381:~$ ping 10.75.53.3 -s 8900 PING 10.75.53.3 (10.75.53.3) 8900(8928) bytes of data. 8908 bytes from 10.75.53.3: icmp_seq=1 ttl=61 time=1.65 ms 8908 bytes from 10.75.53.3: icmp_seq=2 ttl=61 time=1.58 ms 8908 bytes from 10.75.53.3: icmp_seq=3 ttl=61 time=1.60 ms 8908 bytes from 10.75.53.3: icmp_seq=4 ttl=61 time=2.13 ms 8908 bytes from 10.75.53.3: icmp_seq=5 ttl=61 time=1.58 ms ^C --- 10.75.53.3 ping statistics --- 5 packets transmitted, 5 received, 0% packet loss, time 4006ms rtt min/avg/max/mdev = 1.583/1.713/2.138/0.215 ms ccpuser@mentan-ccp6-master-gro-8a05272381:~$ tracepath -n 10.75.53.3 1?: [LOCALHOST] pmtu 1600 1: 172.16.100.1 0.544ms asymm 2 1: 172.16.100.1 0.348ms asymm 2 2: 192.168.254.2 0.704ms 3: 10.75.53.3 0.957ms reached Resume: pmtu 1600 hops 3 back 4 ccpuser@mentan-ccp6-master-gro-8a05272381:~$ \u4f46\u662f\u95ee\u9898\u4f9d\u65e7\u5b58\u5728\uff01 \u5728Control plane master\u4e0a\u8fdb\u884c\u68c0\u67e5 ccpuser@mentan-ccp6-master1cb612f916:~$ kubectl get all -A NAMESPACE NAME READY STATUS RESTARTS AGE default pod/ccp-api-db-job-y6uxn-8vxkt 0/1 Completed 0 7d12h default pod/ccp-appdata-db-job-20tfm-9s9tb 0/1 Completed 0 7d12h default pod/ccp-db-job-wq2qx-vsfmx 0/1 Completed 0 7d12h default pod/ccp-efk-elasticsearch-curator-1587862800-6jfb6 0/1 Completed 0 40h default pod/ccp-efk-elasticsearch-curator-1587949200-dn5b4 0/1 Completed 0 26h default pod/ccp-efk-elasticsearch-curator-1588035600-sxgtf 0/1 Completed 0 147m default pod/ccp-efk-kibana-66d79b8b8-2jhpz 1/1 Running 2 7d12h default pod/ccp-images-docker-registry-54574d67d4-zzrqp 1/1 Running 2 7d12h default pod/ccp-monitor-grafana-66c7b84c8f-plxx9 1/1 Running 2 7d12h default pod/ccp-monitor-grafana-set-datasource-rf5bb 0/1 Completed 0 7d12h default pod/ccp-monitor-prometheus-alertmanager-55bdd59d7-hkw69 2/2 Running 4 7d12h default pod/ccp-monitor-prometheus-kube-state-metrics-fb9c45fff-trpzc 1/1 Running 3 7d12h default pod/ccp-monitor-prometheus-node-exporter-5r7qg 1/1 Running 2 7d12h default pod/ccp-monitor-prometheus-node-exporter-fbx2j 1/1 Running 2 7d12h default pod/ccp-monitor-prometheus-node-exporter-nrmcf 1/1 Running 2 7d12h default pod/ccp-monitor-prometheus-pass-job-e2mhm-c2k95 0/1 Completed 0 7d12h default pod/ccp-monitor-prometheus-port-update-snq4w-nj2f9 0/1 Completed 0 7d12h default pod/ccp-monitor-prometheus-pushgateway-54ddd4865-ld8zv 1/1 Running 2 7d12h default pod/ccp-monitor-prometheus-server-5f94878c47-4rst7 3/3 Running 6 7d12h default pod/ccp-network-db-job-8lwxw-qt594 0/1 Completed 0 7d12h default pod/ccp-tinker-manager-6668c7555b-c846k 2/2 Running 4 7d12h default pod/cert-manager-85f4bfd857-4jcvr 1/1 Running 2 7d12h default pod/elasticsearch-logging-0 1/1 Running 2 7d12h default pod/fluentd-es-v2.0.2-6tkx2 1/1 Running 2 7d12h default pod/fluentd-es-v2.0.2-sg2zv 1/1 Running 2 7d12h default pod/fluentd-es-v2.0.2-tnl5q 1/1 Running 2 7d12h default pod/hxcsi-csi-attacher-5974bf5479-wlb47 2/2 Running 4 7d12h default pod/hxcsi-csi-nodeplugin-4j7w7 2/2 Running 4 7d12h default pod/hxcsi-csi-nodeplugin-8prvr 2/2 Running 4 7d12h default pod/hxcsi-csi-nodeplugin-f9jnq 2/2 Running 4 7d12h default pod/hxcsi-csi-provisioner-54b4958f8f-fwkqt 2/2 Running 5 7d12h default pod/kaas-api-7dc457d59-hsztr 1/1 Running 2 7d12h default pod/kaas-appdata-67564f8f5b-5x8ct 1/1 Running 2 7d12h default pod/kaas-ccp-aks-operator-74dbcd4985-xvnxx 1/1 Running 3 7d12h default pod/kaas-ccp-cluster-operator-b5fc6b794-rgk9m 1/1 Running 2 7d12h default pod/kaas-ccp-eks-operator-7d6cd6b49b-9jjrn 1/1 Running 2 7d12h default pod/kaas-ccp-vsphere-operator-5646b6b975-tfbtm 1/1 Running 3 7d12h default pod/kaas-corc-6b9fb75bbd-mjszd 1/1 Running 2 7d12h default pod/kaas-cx-aes-key-job-4i3i2-r8dlc 0/1 Completed 0 7d12h default pod/kaas-dashboard-6c56cbd9d-b5s9j 1/1 Running 4 7d12h default pod/kaas-network-d475b45c4-hjxg4 1/1 Running 2 7d12h default pod/kaas-network-d475b45c4-vjrgn 1/1 Running 2 7d12h default pod/kaas-network-initdb-1lzjb-cfjrr 0/1 Completed 0 7d12h default pod/kaas-sddc-55b4f55f99-28h6t 1/1 Running 2 7d12h default pod/kaas-slagent-9558db56c-z75zh 1/1 Running 2 7d12h default pod/kaas-slagent-xt6qz-fhcnw 0/1 Completed 2 7d12h default pod/metallb-controller-598ff9f5c5-f8tbd 1/1 Running 2 7d12h default pod/metallb-speaker-jwppt 1/1 Running 2 7d12h default pod/metallb-speaker-kv4nj 1/1 Running 2 7d12h default pod/metallb-speaker-sm6dg 1/1 Running 2 7d12h default pod/metallb-speaker-tz5qd 1/1 Running 2 7d12h default pod/mysql-0 1/1 Running 3 7d12h default pod/nginx-ingress-controller-4nflt 1/1 Running 2 7d12h default pod/nginx-ingress-controller-mdg85 1/1 Running 2 7d12h default pod/nginx-ingress-controller-qtczf 1/1 Running 2 7d12h default pod/nginx-ingress-default-backend-868c85bf9-46b8r 1/1 Running 2 7d12h default pod/vsphere-cluster-conversion-webhook-66c44fccb9-9kkm9 1/1 Running 2 7d12h default pod/wsrep-pass-chage-job-1wepm-jr9k5 0/1 Completed 0 7d12h kube-system pod/calico-kube-controllers-67878b79d6-4znbr 1/1 Running 2 7d12h kube-system pod/calico-node-c7lbx 1/1 Running 2 7d12h kube-system pod/calico-node-mz8wj 1/1 Running 2 7d12h kube-system pod/calico-node-rw622 1/1 Running 2 7d12h kube-system pod/calico-node-xbgls 1/1 Running 2 7d12h kube-system pod/coredns-775bbfc78d-l799n 1/1 Running 2 7d12h kube-system pod/coredns-775bbfc78d-x4hk4 1/1 Running 2 7d12h kube-system pod/etcd-mentan-ccp6-master1cb612f916 1/1 Running 2 7d12h kube-system pod/kube-apiserver-mentan-ccp6-master1cb612f916 1/1 Running 2 7d12h kube-system pod/kube-controller-manager-mentan-ccp6-master1cb612f916 1/1 Running 2 7d12h kube-system pod/kube-proxy-gw4k5 1/1 Running 2 7d12h kube-system pod/kube-proxy-ljzn8 1/1 Running 2 7d12h kube-system pod/kube-proxy-qggp5 1/1 Running 2 7d12h kube-system pod/kube-proxy-qmtch 1/1 Running 2 7d12h kube-system pod/kube-scheduler-mentan-ccp6-master1cb612f916 1/1 Running 2 7d12h kube-system pod/nvidia-device-plugin-daemonset-lsh6s 1/1 Running 2 7d12h kube-system pod/nvidia-device-plugin-daemonset-nmvnl 1/1 Running 2 7d12h kube-system pod/nvidia-device-plugin-daemonset-zgx4g 1/1 Running 2 7d12h NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/aks-webhook ClusterIP 10.101.123.154 none 443/TCP 7d12h default service/ccp-efk-kibana ClusterIP 10.98.197.209 none 5601/TCP 7d12h default service/ccp-images-docker-registry ClusterIP 10.104.235.178 none 443/TCP 7d12h default service/ccp-monitor-grafana ClusterIP 10.108.39.113 none 80/TCP 7d12h default service/ccp-monitor-prometheus-alertmanager ClusterIP 10.109.97.8 none 80/TCP 7d12h default service/ccp-monitor-prometheus-kube-state-metrics ClusterIP None none 80/TCP 7d12h default service/ccp-monitor-prometheus-node-exporter ClusterIP None none 9100/TCP 7d12h default service/ccp-monitor-prometheus-pushgateway ClusterIP 10.97.132.250 none 9091/TCP 7d12h default service/ccp-monitor-prometheus-server ClusterIP 10.98.116.10 none 443/TCP 7d12h default service/cert-manager ClusterIP 10.102.218.253 none 9402/TCP 7d12h default service/elasticsearch-logging ClusterIP 10.105.180.251 none 9200/TCP 7d12h default service/kaas-api ClusterIP 10.108.54.182 none 8000/TCP 7d12h default service/kaas-appdata ClusterIP 10.99.174.47 none 8000/TCP 7d12h default service/kaas-corc ClusterIP 10.101.117.202 none 8082/TCP 7d12h default service/kaas-dashboard ClusterIP 10.104.36.58 none 8000/TCP 7d12h default service/kaas-network ClusterIP 10.110.19.160 none 3316/TCP 7d12h default service/kaas-sddc ClusterIP 10.104.183.251 none 8087/TCP 7d12h default service/kaas-slagent ClusterIP 10.103.193.128 none 8443/TCP 7d12h default service/kubernetes ClusterIP 10.96.0.1 none 443/TCP 7d12h default service/mysql ClusterIP None none 3306/TCP 7d12h default service/mysql-public ClusterIP 10.106.181.47 none 3306/TCP 7d12h default service/nginx-ingress-controller LoadBalancer 10.111.255.149 10.75.53.2 80:30112/TCP,443:30503/TCP 7d12h default service/nginx-ingress-default-backend ClusterIP 10.101.122.253 none 80/TCP 7d12h default service/vsphere-cluster-conversion-webhook ClusterIP 10.109.26.224 none 443/TCP 7d12h default service/vsphere-webhook-server ClusterIP 10.106.167.166 none 443/TCP 7d12h kube-system service/calico-typha ClusterIP 10.110.102.239 none 5473/TCP 7d12h kube-system service/kube-dns ClusterIP 10.96.0.10 none 53/UDP,53/TCP,9153/TCP 7d12h NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE default daemonset.apps/ccp-monitor-prometheus-node-exporter 3 3 3 3 3 none 7d12h default daemonset.apps/fluentd-es-v2.0.2 3 3 3 3 3 none 7d12h default daemonset.apps/hxcsi-csi-nodeplugin 3 3 3 3 3 none 7d12h default daemonset.apps/metallb-speaker 4 4 4 4 4 beta.kubernetes.io/os=linux 7d12h default daemonset.apps/nginx-ingress-controller 3 3 3 3 3 none 7d12h kube-system daemonset.apps/calico-node 4 4 4 4 4 beta.kubernetes.io/os=linux 7d12h kube-system daemonset.apps/kube-proxy 4 4 4 4 4 beta.kubernetes.io/os=linux 7d12h kube-system daemonset.apps/nvidia-device-plugin-daemonset 3 3 3 3 3 none 7d12h NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE default deployment.apps/ccp-efk-kibana 1/1 1 1 7d12h default deployment.apps/ccp-images-docker-registry 1/1 1 1 7d12h default deployment.apps/ccp-monitor-grafana 1/1 1 1 7d12h default deployment.apps/ccp-monitor-prometheus-alertmanager 1/1 1 1 7d12h default deployment.apps/ccp-monitor-prometheus-kube-state-metrics 1/1 1 1 7d12h default deployment.apps/ccp-monitor-prometheus-pushgateway 1/1 1 1 7d12h default deployment.apps/ccp-monitor-prometheus-server 1/1 1 1 7d12h default deployment.apps/ccp-tinker-manager 1/1 1 1 7d12h default deployment.apps/cert-manager 1/1 1 1 7d12h default deployment.apps/hxcsi-csi-attacher 1/1 1 1 7d12h default deployment.apps/hxcsi-csi-provisioner 1/1 1 1 7d12h default deployment.apps/kaas-api 1/1 1 1 7d12h default deployment.apps/kaas-appdata 1/1 1 1 7d12h default deployment.apps/kaas-ccp-aks-operator 1/1 1 1 7d12h default deployment.apps/kaas-ccp-cluster-operator 1/1 1 1 7d12h default deployment.apps/kaas-ccp-eks-operator 1/1 1 1 7d12h default deployment.apps/kaas-ccp-vsphere-operator 1/1 1 1 7d12h default deployment.apps/kaas-corc 1/1 1 1 7d12h default deployment.apps/kaas-dashboard 1/1 1 1 7d12h default deployment.apps/kaas-network 2/2 2 2 7d12h default deployment.apps/kaas-sddc 1/1 1 1 7d12h default deployment.apps/kaas-slagent 1/1 1 1 7d12h default deployment.apps/metallb-controller 1/1 1 1 7d12h default deployment.apps/nginx-ingress-default-backend 1/1 1 1 7d12h default deployment.apps/vsphere-cluster-conversion-webhook 1/1 1 1 7d12h kube-system deployment.apps/calico-kube-controllers 1/1 1 1 7d12h kube-system deployment.apps/calico-typha 0/0 0 0 7d12h kube-system deployment.apps/coredns 2/2 2 2 7d12h NAMESPACE NAME DESIRED CURRENT READY AGE default replicaset.apps/ccp-efk-kibana-66d79b8b8 1 1 1 7d12h default replicaset.apps/ccp-images-docker-registry-54574d67d4 1 1 1 7d12h default replicaset.apps/ccp-monitor-grafana-66c7b84c8f 1 1 1 7d12h default replicaset.apps/ccp-monitor-prometheus-alertmanager-55bdd59d7 1 1 1 7d12h default replicaset.apps/ccp-monitor-prometheus-kube-state-metrics-fb9c45fff 1 1 1 7d12h default replicaset.apps/ccp-monitor-prometheus-pushgateway-54ddd4865 1 1 1 7d12h default replicaset.apps/ccp-monitor-prometheus-server-5f94878c47 1 1 1 7d12h default replicaset.apps/ccp-tinker-manager-6668c7555b 1 1 1 7d12h default replicaset.apps/cert-manager-85f4bfd857 1 1 1 7d12h default replicaset.apps/hxcsi-csi-attacher-5974bf5479 1 1 1 7d12h default replicaset.apps/hxcsi-csi-provisioner-54b4958f8f 1 1 1 7d12h default replicaset.apps/kaas-api-7dc457d59 1 1 1 7d12h default replicaset.apps/kaas-appdata-67564f8f5b 1 1 1 7d12h default replicaset.apps/kaas-ccp-aks-operator-74dbcd4985 1 1 1 7d12h default replicaset.apps/kaas-ccp-cluster-operator-b5fc6b794 1 1 1 7d12h default replicaset.apps/kaas-ccp-eks-operator-7d6cd6b49b 1 1 1 7d12h default replicaset.apps/kaas-ccp-vsphere-operator-5646b6b975 1 1 1 7d12h default replicaset.apps/kaas-corc-6b9fb75bbd 1 1 1 7d12h default replicaset.apps/kaas-dashboard-6c56cbd9d 1 1 1 7d12h default replicaset.apps/kaas-network-d475b45c4 2 2 2 7d12h default replicaset.apps/kaas-sddc-55b4f55f99 1 1 1 7d12h default replicaset.apps/kaas-slagent-9558db56c 1 1 1 7d12h default replicaset.apps/metallb-controller-598ff9f5c5 1 1 1 7d12h default replicaset.apps/nginx-ingress-default-backend-868c85bf9 1 1 1 7d12h default replicaset.apps/vsphere-cluster-conversion-webhook-66c44fccb9 1 1 1 7d12h kube-system replicaset.apps/calico-kube-controllers-67878b79d6 1 1 1 7d12h kube-system replicaset.apps/calico-typha-6757466f55 0 0 0 7d12h kube-system replicaset.apps/coredns-558c8c5f76 0 0 0 7d12h kube-system replicaset.apps/coredns-65f5bff896 0 0 0 7d12h kube-system replicaset.apps/coredns-775bbfc78d 2 2 2 7d12h NAMESPACE NAME READY AGE default statefulset.apps/elasticsearch-logging 1/1 7d12h default statefulset.apps/mysql 1/1 7d12h NAMESPACE NAME COMPLETIONS DURATION AGE default job.batch/ccp-api-db-job-y6uxn 1/1 70s 7d12h default job.batch/ccp-appdata-db-job-20tfm 1/1 69s 7d12h default job.batch/ccp-db-job-wq2qx 1/1 70s 7d12h default job.batch/ccp-efk-elasticsearch-curator-1587862800 1/1 54s 40h default job.batch/ccp-efk-elasticsearch-curator-1587949200 1/1 3s 26h default job.batch/ccp-efk-elasticsearch-curator-1588035600 1/1 3s 147m default job.batch/ccp-monitor-grafana-set-datasource 1/1 47s 7d12h default job.batch/ccp-monitor-prometheus-pass-job-e2mhm 1/1 34s 7d12h default job.batch/ccp-monitor-prometheus-port-update-snq4w 1/1 4s 7d12h default job.batch/ccp-network-db-job-8lwxw 1/1 69s 7d12h default job.batch/kaas-cx-aes-key-job-4i3i2 1/1 51s 7d12h default job.batch/kaas-network-initdb-1lzjb 1/1 20s 7d12h default job.batch/kaas-slagent-xt6qz 1/1 2m30s 7d12h default job.batch/wsrep-pass-chage-job-1wepm 1/1 69s 7d12h NAMESPACE NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE default cronjob.batch/ccp-efk-elasticsearch-curator 0 1 * * * False 0 147m 7d12h NAMESPACE NAME AGE default certmanager.app.ccp.cisco.com/mentan-ccp6 12h NAMESPACE NAME ALLOCATOR STATE ADDRESS default ipaddress.net.ccp.cisco.com/mentan-ccp6-master-gro-e42c4c8448-ens192 mentan-ccp6-allocator Ready 172.16.100.12 default ipaddress.net.ccp.cisco.com/mentan-ccp6-node-group-6316fa20bc-ens192 mentan-ccp6-allocator Ready 172.16.100.13 default ipaddress.net.ccp.cisco.com/mentan-ccp6-vip mentan-ccp6-allocator Ready 172.16.100.11 NAMESPACE NAME CLUSTERNETWORK ROLE READY default netconfig.net.ccp.cisco.com/mentan-ccp6-master-gro-e42c4c8448 mentan-ccp6 master true default netconfig.net.ccp.cisco.com/mentan-ccp6-node-group-6316fa20bc mentan-ccp6 worker true NAMESPACE NAME STATE default cni.net.ccp.cisco.com/mentan-ccp6-cni Pending NAMESPACE NAME SOURCE STATE FREE default ipallocator.net.ccp.cisco.com/mentan-ccp6-allocator ccpnetwork Ready 114 NAMESPACE NAME STATE default nginxingress.net.ccp.cisco.com/mentan-ccp6-ingress Error NAMESPACE NAME STATE default clusternetwork.net.ccp.cisco.com/mentan-ccp6 ClusterNetworkError \u68c0\u67e5syslog\uff0c\u53d1\u73b0\u5f88\u591aerror tail -f /var/log/syslog ... Apr 28 03:45:26 mentan-ccp6-master1cb612f916 kubelet[1218]: E0428 03:45:26.787813 1218 summary_sys_containers.go:47] Failed to get system container stats for \"/system.slice/docker.service\": failed to get cgroup stats for \"/system.slice/docker.service\": failed to get container info for \"/system.slice/docker.service\": unknown container \"/system.slice/docker.service\" Apr 28 03:45:33 mentan-ccp6-master1cb612f916 bash[1136]: RTNETLINK answers: File exists Apr 28 03:45:36 mentan-ccp6-master1cb612f916 kubelet[1218]: E0428 03:45:36.825205 1218 summary_sys_containers.go:47] Failed to get system container stats for \"/system.slice/docker.service\": failed to get cgroup stats for \"/system.slice/docker.service\": failed to get container info for \"/system.slice/docker.service\": unknown container \"/system.slice/docker.service\" Apr 28 03:45:46 mentan-ccp6-master1cb612f916 kubelet[1218]: E0428 03:45:46.858089 1218 summary_sys_containers.go:47] Failed to get system container stats for \"/system.slice/docker.service\": failed to get cgroup stats for \"/system.slice/docker.service\": failed to get container info for \"/system.slice/docker.service\": unknown container \"/system.slice/docker.service\" Apr 28 03:45:56 mentan-ccp6-master1cb612f916 kubelet[1218]: E0428 03:45:56.892498 1218 summary_sys_containers.go:47] Failed to get system container stats for \"/system.slice/docker.service\": failed to get cgroup stats for \"/system.slice/docker.service\": failed to get container info for \"/system.slice/docker.service\": unknown container \"/system.slice/docker.service\"","title":"CCP\u6392\u969c"},{"location":"ccp/ccp-troubleshooting/#ccp","text":"","title":"CCP \u6392\u969c"},{"location":"ccp/ccp-troubleshooting/#issue-1-coredns-pod-stuck-in-pending","text":"\u6545\u969c\u73b0\u8c61\uff1a coredns pod\u5904\u4e8epending\u72b6\u6001\u5bfc\u81f4master node\u90e8\u7f72\u5931\u8d25 Cluster\u4e2d\u7684master node\u90e8\u7f72\u5931\u8d25 \u6392\u969c\u8fc7\u7a0b Note: CCP\u90e8\u7f72\u7684node\u662fminimal\u7684ubuntu\uff0c\u4e0d\u5305\u542bping\u7b49\u547d\u4ee4\uff0c\u53ef\u4ee5\u4f7f\u7528\u4e0b\u9762\u7684\u65b9\u6cd5\u52a0\u5165ping\u5e94\u7528 apt-get update apt-get install iputils-ping \u68c0\u67e5coredns log\uff0c\u6ca1\u6709\u8f93\u51fa ccpuser@mentan-ccp6-master-gro-6e6734d30b:~$ ccpuser@mentan-ccp6-master-gro-6e6734d30b:~$ kubectl logs -f coredns-65f5bff896-wz9hb -n kube-system ccpuser@mentan-ccp6-master-gro-6e6734d30b:~$ \u67e5\u770bcoredns pod\u8be6\u7ec6\u4fe1\u606f ccpuser@mentan-ccp6-master-gro-6e6734d30b:~$ kubectl describe pod coredns-65f5bff896-wz9hb -n kube-system Name: coredns-65f5bff896-wz9hb Namespace: kube-system Priority: 2000000000 Priority Class Name: system-cluster-critical Node: none Labels: k8s-app=kube-dns pod-template-hash=65f5bff896 Annotations: none Status: Pending IP: IPs: none Controlled By: ReplicaSet/coredns-65f5bff896 Containers: coredns: Image: registry.ci.ciscolabs.com/cpsg_ccp/k8s.gcr.io/coredns:1.6.2 Ports: 53/UDP, 53/TCP, 9153/TCP Host Ports: 0/UDP, 0/TCP, 0/TCP Args: -conf /etc/coredns/Corefile Limits: memory: 170Mi Requests: cpu: 100m memory: 70Mi Liveness: http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5 Readiness: http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3 Environment: none Mounts: /etc/coredns from config-volume (ro) /var/run/secrets/kubernetes.io/serviceaccount from coredns-token-z9hqz (ro) Conditions: Type Status PodScheduled False Volumes: config-volume: Type: ConfigMap (a volume populated by a ConfigMap) Name: coredns Optional: false coredns-token-z9hqz: Type: Secret (a volume populated by a Secret) SecretName: coredns-token-z9hqz Optional: false QoS Class: Burstable Node-Selectors: beta.kubernetes.io/os=linux Tolerations: CriticalAddonsOnly node-role.kubernetes.io/master:NoSchedule node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 89s (x297 over 7h22m) default-scheduler 0/1 nodes are available: 1 node(s) had taints that the pod didn't tolerate. ccpuser@mentan-ccp6-master-gro-6e6734d30b:~$ \u68c0\u67e5coredns ConfigMap kubectl edit cm coredns -n kube-system # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 data: Corefile: \".:53 {\\n errors\\n health\\n ready\\n kubernetes cluster.local in-addr.arpa ip6.arpa {\\n pods insecure\\n fallthrough in-addr.arpa ip6.arpa\\n ttl 30\\n }\\n prometheus :9153\\n forward . /etc/resolv.conf\\n \\ cache 30\\n \\n reload\\n loadbalance\\n}\\n\" kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"v1\",\"data\":{\"Corefile\":\".:53 {\\n errors\\n health\\n ready\\n kubernetes cluster.local in-addr.arpa ip6.arpa {\\n pods insecure\\n fallthrough in-addr.arpa ip6.arpa\\n ttl 30\\n }\\n prometheus :9153\\n forward . /etc/resolv.conf\\n cache 30\\n \\n reload\\n loadbalance\\n}\\n\"},\"kind\":\"ConfigMap\",\"metadata\":{\"annotations\":{},\"creationTimestamp\":\"2020-04-16T16:58:00Z\",\"name\":\"coredns\",\"namespace\":\"kube-system\",\"resourceVersion\":\"175\",\"selfLink\":\"/api/v1/namespaces/kube-system/configmaps/coredns\",\"uid\":\"3ed0b0e3-640a-4dda-98ff-8fc897e159c4\"}} creationTimestamp: \"2020-04-16T16:58:00Z\" name: coredns namespace: kube-system resourceVersion: \"224\" selfLink: /api/v1/namespaces/kube-system/configmaps/coredns uid: 3ed0b0e3-640a-4dda-98ff-8fc897e159c4 \u68c0\u67e5resolv.conf\u914d\u7f6e ccpuser@mentan-ccp6-master-gro-6e6734d30b:~$ more /etc/resolv.conf # Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8) # DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN # 127.0.0.53 is the systemd-resolved stub resolver. # run \"systemd-resolve --status\" to see details about the actual nameservers. nameserver 64.104.123.245 \u901a\u8fc7nslookup\u68c0\u67e5\u4e3b\u673aDNS\uff0c\u4e00\u5207\u6b63\u5e38\u3002 \u68c0\u67e5CCP\u5b89\u88c5\u65e5\u5fd7, \u6709\u5f88\u591a\"network plugin is not ready\"\u7684\u9519\u8bef more /var/log/syslog ... Apr 20 03:16:19 mentan-ccp6-master-gro-fedf9b35d0 kubelet[3358]: W0420 03:16:19.212243 3358 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d Apr 20 03:16:23 mentan-ccp6-master-gro-fedf9b35d0 kubelet[3358]: E0420 03:16:23.267608 3358 kubelet.go:2187] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized Apr 20 03:16:24 mentan-ccp6-master-gro-fedf9b35d0 kubelet[3358]: W0420 03:16:24.212638 3358 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d Apr 20 03:16:24 mentan-ccp6-master-gro-fedf9b35d0 systemd[1]: Started Session 21 of user ccpuser. Apr 20 03:16:28 mentan-ccp6-master-gro-fedf9b35d0 kubelet[3358]: E0420 03:16:28.269207 3358 kubelet.go:2187] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized Apr 20 03:16:29 mentan-ccp6-master-gro-fedf9b35d0 kubelet[3358]: W0420 03:16:29.213033 3358 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d \u68c0\u67e5HX MTU, \u5df2\u7ecf\u88ab\u8bbe\u7f6e\u4e3a9216 ccpuser@mentan-ccp6-master-gro-fedf9b35d0:~$ kubectl describe node Name: mentan-ccp6-master-gro-fedf9b35d0 Roles: master Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/arch=amd64 kubernetes.io/hostname=mentan-ccp6-master-gro-fedf9b35d0 kubernetes.io/os=linux node-role.kubernetes.io/master= Annotations: kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock node.alpha.kubernetes.io/ttl: 0 volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Mon, 20 Apr 2020 02:05:54 +0000 Taints: node-role.kubernetes.io/master:NoSchedule node.kubernetes.io/not-ready:NoSchedule Unschedulable: false Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Mon, 20 Apr 2020 07:08:03 +0000 Mon, 20 Apr 2020 02:05:50 +0000 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Mon, 20 Apr 2020 07:08:03 +0000 Mon, 20 Apr 2020 02:05:50 +0000 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Mon, 20 Apr 2020 07:08:03 +0000 Mon, 20 Apr 2020 02:05:50 +0000 KubeletHasSufficientPID kubelet has sufficient PID available Ready False Mon, 20 Apr 2020 07:08:03 +0000 Mon, 20 Apr 2020 02:05:50 +0000 KubeletNotReady runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized Addresses: ExternalIP: 192.168.100.10 InternalIP: 192.168.100.10 Hostname: mentan-ccp6-master-gro-fedf9b35d0 Capacity: cpu: 2 ephemeral-storage: 40470732Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 16426480Ki pods: 110 Allocatable: cpu: 2 ephemeral-storage: 37297826550 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 16324080Ki pods: 110 System Info: Machine ID: 93c34496f87f4a39b4b6d51766ee71a7 System UUID: 4203BCE9-816E-EB82-BDF0-97599E3F9731 Boot ID: 2479c7d3-3fe9-4112-b5f6-d2a4b1f7e54a Kernel Version: 4.15.0-64-generic OS Image: Ubuntu 18.04.3 LTS Operating System: linux Architecture: amd64 Container Runtime Version: docker://18.9.9 Kubelet Version: v1.16.3 Kube-Proxy Version: v1.16.3 PodCIDR: 10.51.0.0/24 PodCIDRs: 10.51.0.0/24 ProviderID: vsphere://4203bce9-816e-eb82-bdf0-97599e3f9731 Non-terminated Pods: (6 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- ccp ccp-vip-manager-mentan-ccp6-master-gro-fedf9b35d0 0 (0%) 0 (0%) 0 (0%) 0 (0%) 5h1m kube-system etcd-mentan-ccp6-master-gro-fedf9b35d0 0 (0%) 0 (0%) 0 (0%) 0 (0%) 5h1m kube-system kube-apiserver-mentan-ccp6-master-gro-fedf9b35d0 250m (12%) 0 (0%) 0 (0%) 0 (0%) 5h1m kube-system kube-controller-manager-mentan-ccp6-master-gro-fedf9b35d0 200m (10%) 0 (0%) 0 (0%) 0 (0%) 5h1m kube-system kube-proxy-xpdff 0 (0%) 0 (0%) 0 (0%) 0 (0%) 5h2m kube-system kube-scheduler-mentan-ccp6-master-gro-fedf9b35d0 100m (5%) 0 (0%) 0 (0%) 0 (0%) 5h1m Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 550m (27%) 0 (0%) memory 0 (0%) 0 (0%) ephemeral-storage 0 (0%) 0 (0%) Events: none \u4ecegroup master \u7528Jumbo packet ping control-plane master, \u53d1\u73b0\u4e22\u5305 ccpuser@mentan-ccp6-master-gro-8a05272381:~$ tracepath -n 10.75.53.3 1?: [LOCALHOST] pmtu 1600 1: 172.16.100.1 0.576ms asymm 2 1: 172.16.100.1 0.416ms asymm 2 2: 192.168.254.2 0.666ms 3: no reply 4: no reply 5: no reply 6: no reply 7: no reply 8: no reply 9: no reply 10: no reply 11: no reply 12: no reply 13: no reply 14: no reply 15: no reply 16: no reply 17: no reply 18: no reply 19: no reply 20: no reply 21: no reply 22: no reply 23: no reply 24: no reply 25: no reply 26: no reply 27: no reply 28: no reply 29: no reply 30: no reply Too many hops: pmtu 1600 Resume: pmtu 1600 ccpuser@mentan-ccp6-master-gro-8a05272381:~$ \u7ecf\u8fc7\u68b3\u7406lab\u62d3\u6251\uff0c\u53d1\u73b0\u53ef\u80fd\u5b58\u5728MTU\u7684\u95ee\u9898\u5728\u4e0a\u884c\u7684OOB 4948 switch \u6839\u636e\u4e0b\u9762\u76844500 Jumbo frame\u6392\u9519\u6587\u6863\uff0c\u5c06OOB 4948 switch\u7684MTU\u8c03\u6574\u4e3a9000 https://www.cisco.com/c/en/us/support/docs/switches/catalyst-4000-series-switches/29805-175.html OOB 4948 MTU\u72b6\u6001\u5982\u4e0b ACI_Fabric2_OOB#show vlan mtu VLAN SVI_MTU MinMTU(port) MaxMTU(port) MTU_Mismatch ---- ------------- ---------------- --------------- ------------ 1 9000 9000 9000 No 1002 - 9000 9000 No 1003 - 9000 9000 No 1004 - 9000 9000 No 1005 - 9000 9000 No ACI_Fabric2_OOB#show interfaces mtu Port Name MTU Gi1/1 9000 Gi1/2 9000 Gi1/3 9000 Gi1/4 9000 Gi1/5 9000 Gi1/6 9000 Gi1/7 9000 Gi1/8 9000 Gi1/9 9000 Gi1/10 9000 Gi1/11 9000 Gi1/12 9000 Gi1/13 9000 Gi1/14 9000 Gi1/15 9000 Gi1/16 9000 Gi1/17 9000 Gi1/18 9000 Gi1/19 9000 Gi1/20 9000 Gi1/21 9000 Gi1/22 9000 Gi1/23 9000 Gi1/24 9000 Gi1/25 9000 Gi1/26 9000 Gi1/27 9000 Gi1/28 9000 Gi1/29 9000 Gi1/30 9000 Gi1/31 9000 Gi1/32 9000 Gi1/33 9000 Gi1/34 9000 Gi1/35 9000 Gi1/36 9000 Gi1/37 9000 Gi1/38 9000 Gi1/39 9000 Gi1/40 9000 Gi1/41 9000 Gi1/42 9000 Gi1/43 9000 Gi1/44 9000 Gi1/45 9000 Gi1/46 9000 Gi1/47 9000 Gi1/48 9000 Te1/49 9000 Te1/50 9000 ACI_Fabric2_OOB# \u7ecf\u8fc7\u4e0a\u8ff0\u8c03\u6574\u540e\uff0c\u53d1\u73b0MTU\u95ee\u9898\u89e3\u51b3 ccpuser@mentan-ccp6-master-gro-8a05272381:~$ ping 10.75.53.3 -s 8900 PING 10.75.53.3 (10.75.53.3) 8900(8928) bytes of data. 8908 bytes from 10.75.53.3: icmp_seq=1 ttl=61 time=1.65 ms 8908 bytes from 10.75.53.3: icmp_seq=2 ttl=61 time=1.58 ms 8908 bytes from 10.75.53.3: icmp_seq=3 ttl=61 time=1.60 ms 8908 bytes from 10.75.53.3: icmp_seq=4 ttl=61 time=2.13 ms 8908 bytes from 10.75.53.3: icmp_seq=5 ttl=61 time=1.58 ms ^C --- 10.75.53.3 ping statistics --- 5 packets transmitted, 5 received, 0% packet loss, time 4006ms rtt min/avg/max/mdev = 1.583/1.713/2.138/0.215 ms ccpuser@mentan-ccp6-master-gro-8a05272381:~$ tracepath -n 10.75.53.3 1?: [LOCALHOST] pmtu 1600 1: 172.16.100.1 0.544ms asymm 2 1: 172.16.100.1 0.348ms asymm 2 2: 192.168.254.2 0.704ms 3: 10.75.53.3 0.957ms reached Resume: pmtu 1600 hops 3 back 4 ccpuser@mentan-ccp6-master-gro-8a05272381:~$ \u4f46\u662f\u95ee\u9898\u4f9d\u65e7\u5b58\u5728\uff01 \u5728Control plane master\u4e0a\u8fdb\u884c\u68c0\u67e5 ccpuser@mentan-ccp6-master1cb612f916:~$ kubectl get all -A NAMESPACE NAME READY STATUS RESTARTS AGE default pod/ccp-api-db-job-y6uxn-8vxkt 0/1 Completed 0 7d12h default pod/ccp-appdata-db-job-20tfm-9s9tb 0/1 Completed 0 7d12h default pod/ccp-db-job-wq2qx-vsfmx 0/1 Completed 0 7d12h default pod/ccp-efk-elasticsearch-curator-1587862800-6jfb6 0/1 Completed 0 40h default pod/ccp-efk-elasticsearch-curator-1587949200-dn5b4 0/1 Completed 0 26h default pod/ccp-efk-elasticsearch-curator-1588035600-sxgtf 0/1 Completed 0 147m default pod/ccp-efk-kibana-66d79b8b8-2jhpz 1/1 Running 2 7d12h default pod/ccp-images-docker-registry-54574d67d4-zzrqp 1/1 Running 2 7d12h default pod/ccp-monitor-grafana-66c7b84c8f-plxx9 1/1 Running 2 7d12h default pod/ccp-monitor-grafana-set-datasource-rf5bb 0/1 Completed 0 7d12h default pod/ccp-monitor-prometheus-alertmanager-55bdd59d7-hkw69 2/2 Running 4 7d12h default pod/ccp-monitor-prometheus-kube-state-metrics-fb9c45fff-trpzc 1/1 Running 3 7d12h default pod/ccp-monitor-prometheus-node-exporter-5r7qg 1/1 Running 2 7d12h default pod/ccp-monitor-prometheus-node-exporter-fbx2j 1/1 Running 2 7d12h default pod/ccp-monitor-prometheus-node-exporter-nrmcf 1/1 Running 2 7d12h default pod/ccp-monitor-prometheus-pass-job-e2mhm-c2k95 0/1 Completed 0 7d12h default pod/ccp-monitor-prometheus-port-update-snq4w-nj2f9 0/1 Completed 0 7d12h default pod/ccp-monitor-prometheus-pushgateway-54ddd4865-ld8zv 1/1 Running 2 7d12h default pod/ccp-monitor-prometheus-server-5f94878c47-4rst7 3/3 Running 6 7d12h default pod/ccp-network-db-job-8lwxw-qt594 0/1 Completed 0 7d12h default pod/ccp-tinker-manager-6668c7555b-c846k 2/2 Running 4 7d12h default pod/cert-manager-85f4bfd857-4jcvr 1/1 Running 2 7d12h default pod/elasticsearch-logging-0 1/1 Running 2 7d12h default pod/fluentd-es-v2.0.2-6tkx2 1/1 Running 2 7d12h default pod/fluentd-es-v2.0.2-sg2zv 1/1 Running 2 7d12h default pod/fluentd-es-v2.0.2-tnl5q 1/1 Running 2 7d12h default pod/hxcsi-csi-attacher-5974bf5479-wlb47 2/2 Running 4 7d12h default pod/hxcsi-csi-nodeplugin-4j7w7 2/2 Running 4 7d12h default pod/hxcsi-csi-nodeplugin-8prvr 2/2 Running 4 7d12h default pod/hxcsi-csi-nodeplugin-f9jnq 2/2 Running 4 7d12h default pod/hxcsi-csi-provisioner-54b4958f8f-fwkqt 2/2 Running 5 7d12h default pod/kaas-api-7dc457d59-hsztr 1/1 Running 2 7d12h default pod/kaas-appdata-67564f8f5b-5x8ct 1/1 Running 2 7d12h default pod/kaas-ccp-aks-operator-74dbcd4985-xvnxx 1/1 Running 3 7d12h default pod/kaas-ccp-cluster-operator-b5fc6b794-rgk9m 1/1 Running 2 7d12h default pod/kaas-ccp-eks-operator-7d6cd6b49b-9jjrn 1/1 Running 2 7d12h default pod/kaas-ccp-vsphere-operator-5646b6b975-tfbtm 1/1 Running 3 7d12h default pod/kaas-corc-6b9fb75bbd-mjszd 1/1 Running 2 7d12h default pod/kaas-cx-aes-key-job-4i3i2-r8dlc 0/1 Completed 0 7d12h default pod/kaas-dashboard-6c56cbd9d-b5s9j 1/1 Running 4 7d12h default pod/kaas-network-d475b45c4-hjxg4 1/1 Running 2 7d12h default pod/kaas-network-d475b45c4-vjrgn 1/1 Running 2 7d12h default pod/kaas-network-initdb-1lzjb-cfjrr 0/1 Completed 0 7d12h default pod/kaas-sddc-55b4f55f99-28h6t 1/1 Running 2 7d12h default pod/kaas-slagent-9558db56c-z75zh 1/1 Running 2 7d12h default pod/kaas-slagent-xt6qz-fhcnw 0/1 Completed 2 7d12h default pod/metallb-controller-598ff9f5c5-f8tbd 1/1 Running 2 7d12h default pod/metallb-speaker-jwppt 1/1 Running 2 7d12h default pod/metallb-speaker-kv4nj 1/1 Running 2 7d12h default pod/metallb-speaker-sm6dg 1/1 Running 2 7d12h default pod/metallb-speaker-tz5qd 1/1 Running 2 7d12h default pod/mysql-0 1/1 Running 3 7d12h default pod/nginx-ingress-controller-4nflt 1/1 Running 2 7d12h default pod/nginx-ingress-controller-mdg85 1/1 Running 2 7d12h default pod/nginx-ingress-controller-qtczf 1/1 Running 2 7d12h default pod/nginx-ingress-default-backend-868c85bf9-46b8r 1/1 Running 2 7d12h default pod/vsphere-cluster-conversion-webhook-66c44fccb9-9kkm9 1/1 Running 2 7d12h default pod/wsrep-pass-chage-job-1wepm-jr9k5 0/1 Completed 0 7d12h kube-system pod/calico-kube-controllers-67878b79d6-4znbr 1/1 Running 2 7d12h kube-system pod/calico-node-c7lbx 1/1 Running 2 7d12h kube-system pod/calico-node-mz8wj 1/1 Running 2 7d12h kube-system pod/calico-node-rw622 1/1 Running 2 7d12h kube-system pod/calico-node-xbgls 1/1 Running 2 7d12h kube-system pod/coredns-775bbfc78d-l799n 1/1 Running 2 7d12h kube-system pod/coredns-775bbfc78d-x4hk4 1/1 Running 2 7d12h kube-system pod/etcd-mentan-ccp6-master1cb612f916 1/1 Running 2 7d12h kube-system pod/kube-apiserver-mentan-ccp6-master1cb612f916 1/1 Running 2 7d12h kube-system pod/kube-controller-manager-mentan-ccp6-master1cb612f916 1/1 Running 2 7d12h kube-system pod/kube-proxy-gw4k5 1/1 Running 2 7d12h kube-system pod/kube-proxy-ljzn8 1/1 Running 2 7d12h kube-system pod/kube-proxy-qggp5 1/1 Running 2 7d12h kube-system pod/kube-proxy-qmtch 1/1 Running 2 7d12h kube-system pod/kube-scheduler-mentan-ccp6-master1cb612f916 1/1 Running 2 7d12h kube-system pod/nvidia-device-plugin-daemonset-lsh6s 1/1 Running 2 7d12h kube-system pod/nvidia-device-plugin-daemonset-nmvnl 1/1 Running 2 7d12h kube-system pod/nvidia-device-plugin-daemonset-zgx4g 1/1 Running 2 7d12h NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/aks-webhook ClusterIP 10.101.123.154 none 443/TCP 7d12h default service/ccp-efk-kibana ClusterIP 10.98.197.209 none 5601/TCP 7d12h default service/ccp-images-docker-registry ClusterIP 10.104.235.178 none 443/TCP 7d12h default service/ccp-monitor-grafana ClusterIP 10.108.39.113 none 80/TCP 7d12h default service/ccp-monitor-prometheus-alertmanager ClusterIP 10.109.97.8 none 80/TCP 7d12h default service/ccp-monitor-prometheus-kube-state-metrics ClusterIP None none 80/TCP 7d12h default service/ccp-monitor-prometheus-node-exporter ClusterIP None none 9100/TCP 7d12h default service/ccp-monitor-prometheus-pushgateway ClusterIP 10.97.132.250 none 9091/TCP 7d12h default service/ccp-monitor-prometheus-server ClusterIP 10.98.116.10 none 443/TCP 7d12h default service/cert-manager ClusterIP 10.102.218.253 none 9402/TCP 7d12h default service/elasticsearch-logging ClusterIP 10.105.180.251 none 9200/TCP 7d12h default service/kaas-api ClusterIP 10.108.54.182 none 8000/TCP 7d12h default service/kaas-appdata ClusterIP 10.99.174.47 none 8000/TCP 7d12h default service/kaas-corc ClusterIP 10.101.117.202 none 8082/TCP 7d12h default service/kaas-dashboard ClusterIP 10.104.36.58 none 8000/TCP 7d12h default service/kaas-network ClusterIP 10.110.19.160 none 3316/TCP 7d12h default service/kaas-sddc ClusterIP 10.104.183.251 none 8087/TCP 7d12h default service/kaas-slagent ClusterIP 10.103.193.128 none 8443/TCP 7d12h default service/kubernetes ClusterIP 10.96.0.1 none 443/TCP 7d12h default service/mysql ClusterIP None none 3306/TCP 7d12h default service/mysql-public ClusterIP 10.106.181.47 none 3306/TCP 7d12h default service/nginx-ingress-controller LoadBalancer 10.111.255.149 10.75.53.2 80:30112/TCP,443:30503/TCP 7d12h default service/nginx-ingress-default-backend ClusterIP 10.101.122.253 none 80/TCP 7d12h default service/vsphere-cluster-conversion-webhook ClusterIP 10.109.26.224 none 443/TCP 7d12h default service/vsphere-webhook-server ClusterIP 10.106.167.166 none 443/TCP 7d12h kube-system service/calico-typha ClusterIP 10.110.102.239 none 5473/TCP 7d12h kube-system service/kube-dns ClusterIP 10.96.0.10 none 53/UDP,53/TCP,9153/TCP 7d12h NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE default daemonset.apps/ccp-monitor-prometheus-node-exporter 3 3 3 3 3 none 7d12h default daemonset.apps/fluentd-es-v2.0.2 3 3 3 3 3 none 7d12h default daemonset.apps/hxcsi-csi-nodeplugin 3 3 3 3 3 none 7d12h default daemonset.apps/metallb-speaker 4 4 4 4 4 beta.kubernetes.io/os=linux 7d12h default daemonset.apps/nginx-ingress-controller 3 3 3 3 3 none 7d12h kube-system daemonset.apps/calico-node 4 4 4 4 4 beta.kubernetes.io/os=linux 7d12h kube-system daemonset.apps/kube-proxy 4 4 4 4 4 beta.kubernetes.io/os=linux 7d12h kube-system daemonset.apps/nvidia-device-plugin-daemonset 3 3 3 3 3 none 7d12h NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE default deployment.apps/ccp-efk-kibana 1/1 1 1 7d12h default deployment.apps/ccp-images-docker-registry 1/1 1 1 7d12h default deployment.apps/ccp-monitor-grafana 1/1 1 1 7d12h default deployment.apps/ccp-monitor-prometheus-alertmanager 1/1 1 1 7d12h default deployment.apps/ccp-monitor-prometheus-kube-state-metrics 1/1 1 1 7d12h default deployment.apps/ccp-monitor-prometheus-pushgateway 1/1 1 1 7d12h default deployment.apps/ccp-monitor-prometheus-server 1/1 1 1 7d12h default deployment.apps/ccp-tinker-manager 1/1 1 1 7d12h default deployment.apps/cert-manager 1/1 1 1 7d12h default deployment.apps/hxcsi-csi-attacher 1/1 1 1 7d12h default deployment.apps/hxcsi-csi-provisioner 1/1 1 1 7d12h default deployment.apps/kaas-api 1/1 1 1 7d12h default deployment.apps/kaas-appdata 1/1 1 1 7d12h default deployment.apps/kaas-ccp-aks-operator 1/1 1 1 7d12h default deployment.apps/kaas-ccp-cluster-operator 1/1 1 1 7d12h default deployment.apps/kaas-ccp-eks-operator 1/1 1 1 7d12h default deployment.apps/kaas-ccp-vsphere-operator 1/1 1 1 7d12h default deployment.apps/kaas-corc 1/1 1 1 7d12h default deployment.apps/kaas-dashboard 1/1 1 1 7d12h default deployment.apps/kaas-network 2/2 2 2 7d12h default deployment.apps/kaas-sddc 1/1 1 1 7d12h default deployment.apps/kaas-slagent 1/1 1 1 7d12h default deployment.apps/metallb-controller 1/1 1 1 7d12h default deployment.apps/nginx-ingress-default-backend 1/1 1 1 7d12h default deployment.apps/vsphere-cluster-conversion-webhook 1/1 1 1 7d12h kube-system deployment.apps/calico-kube-controllers 1/1 1 1 7d12h kube-system deployment.apps/calico-typha 0/0 0 0 7d12h kube-system deployment.apps/coredns 2/2 2 2 7d12h NAMESPACE NAME DESIRED CURRENT READY AGE default replicaset.apps/ccp-efk-kibana-66d79b8b8 1 1 1 7d12h default replicaset.apps/ccp-images-docker-registry-54574d67d4 1 1 1 7d12h default replicaset.apps/ccp-monitor-grafana-66c7b84c8f 1 1 1 7d12h default replicaset.apps/ccp-monitor-prometheus-alertmanager-55bdd59d7 1 1 1 7d12h default replicaset.apps/ccp-monitor-prometheus-kube-state-metrics-fb9c45fff 1 1 1 7d12h default replicaset.apps/ccp-monitor-prometheus-pushgateway-54ddd4865 1 1 1 7d12h default replicaset.apps/ccp-monitor-prometheus-server-5f94878c47 1 1 1 7d12h default replicaset.apps/ccp-tinker-manager-6668c7555b 1 1 1 7d12h default replicaset.apps/cert-manager-85f4bfd857 1 1 1 7d12h default replicaset.apps/hxcsi-csi-attacher-5974bf5479 1 1 1 7d12h default replicaset.apps/hxcsi-csi-provisioner-54b4958f8f 1 1 1 7d12h default replicaset.apps/kaas-api-7dc457d59 1 1 1 7d12h default replicaset.apps/kaas-appdata-67564f8f5b 1 1 1 7d12h default replicaset.apps/kaas-ccp-aks-operator-74dbcd4985 1 1 1 7d12h default replicaset.apps/kaas-ccp-cluster-operator-b5fc6b794 1 1 1 7d12h default replicaset.apps/kaas-ccp-eks-operator-7d6cd6b49b 1 1 1 7d12h default replicaset.apps/kaas-ccp-vsphere-operator-5646b6b975 1 1 1 7d12h default replicaset.apps/kaas-corc-6b9fb75bbd 1 1 1 7d12h default replicaset.apps/kaas-dashboard-6c56cbd9d 1 1 1 7d12h default replicaset.apps/kaas-network-d475b45c4 2 2 2 7d12h default replicaset.apps/kaas-sddc-55b4f55f99 1 1 1 7d12h default replicaset.apps/kaas-slagent-9558db56c 1 1 1 7d12h default replicaset.apps/metallb-controller-598ff9f5c5 1 1 1 7d12h default replicaset.apps/nginx-ingress-default-backend-868c85bf9 1 1 1 7d12h default replicaset.apps/vsphere-cluster-conversion-webhook-66c44fccb9 1 1 1 7d12h kube-system replicaset.apps/calico-kube-controllers-67878b79d6 1 1 1 7d12h kube-system replicaset.apps/calico-typha-6757466f55 0 0 0 7d12h kube-system replicaset.apps/coredns-558c8c5f76 0 0 0 7d12h kube-system replicaset.apps/coredns-65f5bff896 0 0 0 7d12h kube-system replicaset.apps/coredns-775bbfc78d 2 2 2 7d12h NAMESPACE NAME READY AGE default statefulset.apps/elasticsearch-logging 1/1 7d12h default statefulset.apps/mysql 1/1 7d12h NAMESPACE NAME COMPLETIONS DURATION AGE default job.batch/ccp-api-db-job-y6uxn 1/1 70s 7d12h default job.batch/ccp-appdata-db-job-20tfm 1/1 69s 7d12h default job.batch/ccp-db-job-wq2qx 1/1 70s 7d12h default job.batch/ccp-efk-elasticsearch-curator-1587862800 1/1 54s 40h default job.batch/ccp-efk-elasticsearch-curator-1587949200 1/1 3s 26h default job.batch/ccp-efk-elasticsearch-curator-1588035600 1/1 3s 147m default job.batch/ccp-monitor-grafana-set-datasource 1/1 47s 7d12h default job.batch/ccp-monitor-prometheus-pass-job-e2mhm 1/1 34s 7d12h default job.batch/ccp-monitor-prometheus-port-update-snq4w 1/1 4s 7d12h default job.batch/ccp-network-db-job-8lwxw 1/1 69s 7d12h default job.batch/kaas-cx-aes-key-job-4i3i2 1/1 51s 7d12h default job.batch/kaas-network-initdb-1lzjb 1/1 20s 7d12h default job.batch/kaas-slagent-xt6qz 1/1 2m30s 7d12h default job.batch/wsrep-pass-chage-job-1wepm 1/1 69s 7d12h NAMESPACE NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE default cronjob.batch/ccp-efk-elasticsearch-curator 0 1 * * * False 0 147m 7d12h NAMESPACE NAME AGE default certmanager.app.ccp.cisco.com/mentan-ccp6 12h NAMESPACE NAME ALLOCATOR STATE ADDRESS default ipaddress.net.ccp.cisco.com/mentan-ccp6-master-gro-e42c4c8448-ens192 mentan-ccp6-allocator Ready 172.16.100.12 default ipaddress.net.ccp.cisco.com/mentan-ccp6-node-group-6316fa20bc-ens192 mentan-ccp6-allocator Ready 172.16.100.13 default ipaddress.net.ccp.cisco.com/mentan-ccp6-vip mentan-ccp6-allocator Ready 172.16.100.11 NAMESPACE NAME CLUSTERNETWORK ROLE READY default netconfig.net.ccp.cisco.com/mentan-ccp6-master-gro-e42c4c8448 mentan-ccp6 master true default netconfig.net.ccp.cisco.com/mentan-ccp6-node-group-6316fa20bc mentan-ccp6 worker true NAMESPACE NAME STATE default cni.net.ccp.cisco.com/mentan-ccp6-cni Pending NAMESPACE NAME SOURCE STATE FREE default ipallocator.net.ccp.cisco.com/mentan-ccp6-allocator ccpnetwork Ready 114 NAMESPACE NAME STATE default nginxingress.net.ccp.cisco.com/mentan-ccp6-ingress Error NAMESPACE NAME STATE default clusternetwork.net.ccp.cisco.com/mentan-ccp6 ClusterNetworkError \u68c0\u67e5syslog\uff0c\u53d1\u73b0\u5f88\u591aerror tail -f /var/log/syslog ... Apr 28 03:45:26 mentan-ccp6-master1cb612f916 kubelet[1218]: E0428 03:45:26.787813 1218 summary_sys_containers.go:47] Failed to get system container stats for \"/system.slice/docker.service\": failed to get cgroup stats for \"/system.slice/docker.service\": failed to get container info for \"/system.slice/docker.service\": unknown container \"/system.slice/docker.service\" Apr 28 03:45:33 mentan-ccp6-master1cb612f916 bash[1136]: RTNETLINK answers: File exists Apr 28 03:45:36 mentan-ccp6-master1cb612f916 kubelet[1218]: E0428 03:45:36.825205 1218 summary_sys_containers.go:47] Failed to get system container stats for \"/system.slice/docker.service\": failed to get cgroup stats for \"/system.slice/docker.service\": failed to get container info for \"/system.slice/docker.service\": unknown container \"/system.slice/docker.service\" Apr 28 03:45:46 mentan-ccp6-master1cb612f916 kubelet[1218]: E0428 03:45:46.858089 1218 summary_sys_containers.go:47] Failed to get system container stats for \"/system.slice/docker.service\": failed to get cgroup stats for \"/system.slice/docker.service\": failed to get container info for \"/system.slice/docker.service\": unknown container \"/system.slice/docker.service\" Apr 28 03:45:56 mentan-ccp6-master1cb612f916 kubelet[1218]: E0428 03:45:56.892498 1218 summary_sys_containers.go:47] Failed to get system container stats for \"/system.slice/docker.service\": failed to get cgroup stats for \"/system.slice/docker.service\": failed to get container info for \"/system.slice/docker.service\": unknown container \"/system.slice/docker.service\"","title":"Issue 1\uff1a coredns pod stuck in pending"},{"location":"ccp/ccp-v2-setup/","text":"\u521b\u5efaCCP V2 Cluster \u8bbe\u7f6ev2 ACI CNI Profile 2. \u521b\u5efav2 Cluster finsh... \u5728vcenter\u4e2d\u53ef\u4ee5\u770b\u5230master\u548cwoker\u88ab\u521b\u5efa, \u4ee5\u53ca\u53ef\u7528\u4e8e\u8bbf\u95ee\u7684routable\u5730\u5740 \u901a\u8fc7routable\u5730\u5740\u8bbf\u95eemaster","title":"CCP V2 Cluster \u8bbe\u7f6e"},{"location":"ccp/ccp-v2-setup/#ccp-v2-cluster","text":"\u8bbe\u7f6ev2 ACI CNI Profile 2. \u521b\u5efav2 Cluster finsh... \u5728vcenter\u4e2d\u53ef\u4ee5\u770b\u5230master\u548cwoker\u88ab\u521b\u5efa, \u4ee5\u53ca\u53ef\u7528\u4e8e\u8bbf\u95ee\u7684routable\u5730\u5740 \u901a\u8fc7routable\u5730\u5740\u8bbf\u95eemaster","title":"\u521b\u5efaCCP V2 Cluster"},{"location":"ccp/ccp-v3-setup/","text":"\u521b\u5efaCCP V3 Cluster CCP\u754c\u9762, \u70b9\u51fb\u65b0\u5efaCluster 01-\u8f93\u5165\u57fa\u7840\u4fe1\u606f ACI-CNI Profile\u53c2\u6570 \u57fa\u7840\u4fe1\u606f 02-\u8f93\u5165Provider\u4fe1\u606f 03-\u8f93\u5165Node\u4fe1\u606f 04-\u8f93\u5165ASW IAM\u4fe1\u606f 05-\u5b8c\u6210CCP Cluster\u7684\u521b\u5efa","title":"CCP V3 Cluster \u8bbe\u7f6e"},{"location":"ccp/ccp-v3-setup/#ccp-v3-cluster","text":"CCP\u754c\u9762, \u70b9\u51fb\u65b0\u5efaCluster","title":"\u521b\u5efaCCP V3 Cluster"},{"location":"ccp/ccp-v3-setup/#01-","text":"ACI-CNI Profile\u53c2\u6570 \u57fa\u7840\u4fe1\u606f","title":"01-\u8f93\u5165\u57fa\u7840\u4fe1\u606f"},{"location":"ccp/ccp-v3-setup/#02-provider","text":"","title":"02-\u8f93\u5165Provider\u4fe1\u606f"},{"location":"ccp/ccp-v3-setup/#03-node","text":"","title":"03-\u8f93\u5165Node\u4fe1\u606f"},{"location":"ccp/ccp-v3-setup/#04-asw-iam","text":"","title":"04-\u8f93\u5165ASW IAM\u4fe1\u606f"},{"location":"ccp/ccp-v3-setup/#05-ccp-cluster","text":"","title":"05-\u5b8c\u6210CCP Cluster\u7684\u521b\u5efa"},{"location":"k8s/k8s-cni/","text":"ACI CNI \u96c6\u6210 \u5728master node\u4e0a\u90e8\u7f72ACI CNI Plugin\uff0c 1. \u90e8\u7f72\u5728\u96c6\u6210\u4e2d\u751f\u6210\u7684CNI config YAML\u6587\u4ef6 root@master:~# kubectl apply -f aci-cni-config.yaml configmap/aci-containers-config created secret/aci-user-cert created serviceaccount/aci-containers-controller created serviceaccount/aci-containers-host-agent created clusterrole.rbac.authorization.k8s.io/aci-containers:controller created clusterrole.rbac.authorization.k8s.io/aci-containers:host-agent created clusterrolebinding.rbac.authorization.k8s.io/aci-containers:controller created clusterrolebinding.rbac.authorization.k8s.io/aci-containers:host-agent created daemonset.apps/aci-containers-host created daemonset.apps/aci-containers-openvswitch created deployment.apps/aci-containers-controller created root@master:~# 2. \u76d1\u63a7k8s cluster\u72b6\u6001 root@master:~# kubectl -n kube-system get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES aci-containers-controller-678d948fdb-57gqg 1/1 Running 0 114s 172.16.100.12 worker1 none none aci-containers-host-gsz6c 3/3 Running 0 114s 172.16.100.11 master none none aci-containers-host-qv697 3/3 Running 0 114s 172.16.100.12 worker1 none none aci-containers-openvswitch-qsmqp 1/1 Running 0 114s 172.16.100.11 master none none aci-containers-openvswitch-zvp5w 1/1 Running 0 114s 172.16.100.12 worker1 none none coredns-54ff9cd656-5fcqd 1/1 Running 0 34m 10.2.0.34 worker1 none none coredns-54ff9cd656-6lv9h 1/1 Running 0 34m 10.2.0.35 worker1 none none etcd-master 1/1 Running 0 33m 172.16.100.11 master none none kube-apiserver-master 1/1 Running 0 33m 172.16.100.11 master none none kube-controller-manager-master 1/1 Running 0 33m 172.16.100.11 master none none kube-proxy-l9vs5 1/1 Running 0 25m 172.16.100.12 worker1 none none kube-proxy-ls9nb 1/1 Running 0 34m 172.16.100.11 master none none kube-scheduler-master 1/1 Running 0 33m 172.16.100.11 master none none root@master:~# 3. \u4eceACI\u68c0\u67e5K8s\u72b6\u6001 \u68c0\u67e5Node\u7684Infra IP (\"Infra Tenant\" - access - Default EPG) VMM Domain\u4e2d\u53ef\u4ee5\u770b\u5230node connected \u5728kube-system EPG\u4e2d\u53ef\u4ee5\u770b\u5230KubeDNS node","title":"ACI CNI\u96c6\u6210"},{"location":"k8s/k8s-cni/#aci-cni","text":"\u5728master node\u4e0a\u90e8\u7f72ACI CNI Plugin\uff0c","title":"ACI CNI \u96c6\u6210"},{"location":"k8s/k8s-cni/#1-cni-config-yaml","text":"root@master:~# kubectl apply -f aci-cni-config.yaml configmap/aci-containers-config created secret/aci-user-cert created serviceaccount/aci-containers-controller created serviceaccount/aci-containers-host-agent created clusterrole.rbac.authorization.k8s.io/aci-containers:controller created clusterrole.rbac.authorization.k8s.io/aci-containers:host-agent created clusterrolebinding.rbac.authorization.k8s.io/aci-containers:controller created clusterrolebinding.rbac.authorization.k8s.io/aci-containers:host-agent created daemonset.apps/aci-containers-host created daemonset.apps/aci-containers-openvswitch created deployment.apps/aci-containers-controller created root@master:~#","title":"1. \u90e8\u7f72\u5728\u96c6\u6210\u4e2d\u751f\u6210\u7684CNI config YAML\u6587\u4ef6"},{"location":"k8s/k8s-cni/#2-k8s-cluster","text":"root@master:~# kubectl -n kube-system get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES aci-containers-controller-678d948fdb-57gqg 1/1 Running 0 114s 172.16.100.12 worker1 none none aci-containers-host-gsz6c 3/3 Running 0 114s 172.16.100.11 master none none aci-containers-host-qv697 3/3 Running 0 114s 172.16.100.12 worker1 none none aci-containers-openvswitch-qsmqp 1/1 Running 0 114s 172.16.100.11 master none none aci-containers-openvswitch-zvp5w 1/1 Running 0 114s 172.16.100.12 worker1 none none coredns-54ff9cd656-5fcqd 1/1 Running 0 34m 10.2.0.34 worker1 none none coredns-54ff9cd656-6lv9h 1/1 Running 0 34m 10.2.0.35 worker1 none none etcd-master 1/1 Running 0 33m 172.16.100.11 master none none kube-apiserver-master 1/1 Running 0 33m 172.16.100.11 master none none kube-controller-manager-master 1/1 Running 0 33m 172.16.100.11 master none none kube-proxy-l9vs5 1/1 Running 0 25m 172.16.100.12 worker1 none none kube-proxy-ls9nb 1/1 Running 0 34m 172.16.100.11 master none none kube-scheduler-master 1/1 Running 0 33m 172.16.100.11 master none none root@master:~#","title":"2. \u76d1\u63a7k8s cluster\u72b6\u6001"},{"location":"k8s/k8s-cni/#3-acik8s","text":"\u68c0\u67e5Node\u7684Infra IP (\"Infra Tenant\" - access - Default EPG) VMM Domain\u4e2d\u53ef\u4ee5\u770b\u5230node connected \u5728kube-system EPG\u4e2d\u53ef\u4ee5\u770b\u5230KubeDNS node","title":"3. \u4eceACI\u68c0\u67e5K8s\u72b6\u6001"},{"location":"k8s/k8s-guestbook/","text":"\u90e8\u7f72Guestbook\u5bb9\u5668\u5e94\u7528 \u5728\u672c\u6f14\u793a\u7528\uff0c\u4f7f\u7528\u8457\u540d\u7684Guestbook\u6765\u6f14\u793a\u5bb9\u5668\u5e94\u7528\uff0c\u4ece\u4e0b\u9762\u7684\u94fe\u63a5\u4e0b\u8f7dall-in-one guestbook yaml file\uff1a guestbook-all-in-one.yaml 1. SSH\u767b\u5f55Master Node, \u521b\u5efaguestbook namespace SSH login to master node kubectl create namespace guestbook 2. \u4e0b\u8f7dall-in-one guestbook yaml file, \u90e8\u7f72\u5728guestbook namespace wget https://raw.githubusercontent.com/kubernetes/examples/master/guestbook/all-in-one/guestbook-all-in-one.yaml kubectl -n guestbook apply -f guestbook-all-in-one.yaml \u68c0\u67e5\u72b6\u6001 root@master:~# kubectl -n guestbook get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES frontend-74b4665db5-b9wqh 1/1 Running 0 4h24m 10.2.0.61 worker1 none none frontend-74b4665db5-j6jhw 1/1 Running 0 4h24m 10.2.0.60 worker1 none none frontend-74b4665db5-pzcmx 1/1 Running 0 4h24m 10.2.0.59 worker1 none none redis-master-6fbbc44567-8zn45 1/1 Running 0 4h24m 10.2.0.56 worker1 none none redis-slave-74ccb764fc-b8mjx 1/1 Running 0 4h24m 10.2.0.57 worker1 none none redis-slave-74ccb764fc-kz8j5 1/1 Running 0 4h24m 10.2.0.58 worker1 none none root@master:~# root@master:~# kubectl -n guestbook get svc 3. \u5c06guestbook namespace\u5173\u8054\u81f3\u5355\u72ec\u7684EPG \u624b\u5de5\u521b\u5efa\u65b0\u7684EPG guestbook\uff0c\u9009\u62e9kube-pod-bd\uff0c\u7ee7\u627fkube-default EPG\u7684contract\uff0c\u5173\u8054\u81f3K8s VMM domain \u4f7f\u7528acikubectl\u5de5\u5177\u5c06guestbook namespace\u6dfb\u52a0annotate\u81f3EPG root@master:~# acikubectl set default-eg namespace guestbook -t mykube -a kubernetes -g guestbook Setting default endpoint group: Endpoint Group: Tenant: mykube App profile: kubernetes Endpoint group: guestbook root@master:~# \u6216\u8005\u6807\u51c6\u7684kubectl\u4fee\u6539annotate kubectl annotate namespace guestbook 'opflex.cisco.com/endpoint-group={\"tenant\":\"pov_k8s\",\"app-profile\":\"kubernetes\",\"name\":\"guestbook\"}' \u901a\u8fc7\u4e0b\u9762\u7684\u6307\u4ee4\u68c0\u67e5\u65b0\u7684annotate root@master:~# kubectl describe namespace guestbook Name: guestbook Labels: none Annotations: opflex.cisco.com/endpoint-group: {\"tenant\":\"mykube\",\"app-profile\":\"kubernetes\",\"name\":\"guestbook\"} Status: Active \u4eceACI\u8fdb\u884c\u68c0\u67e5\uff0c\u53d1\u73b0guestbook\u76f8\u5173\u7684pod\u4ecekube-default\u8f6c\u79fb\u5230\u4e86\u65b0\u5efa\u7684guestbook EPG 4. \u5bf9\u5916\u66b4\u9732guestbook\u670d\u52a1 \u5148\u68c0\u67e5ACI CNI\u81ea\u52a8\u521b\u5efa\u7684L4-7 service graph\u914d\u7f6e In common tenant, \u81ea\u52a8\u521b\u5efaL4-7 device service BD\u7528\u4e8ePBR service graph template \u4e0b\u9762\u5f00\u59cb\u8fdb\u884c\u914d\u7f6e\u5bf9\u5916\u66b4\u9732guestbook service \u5c06frontend ServiceType\u7531\u7f3a\u7701\u7684NodePort\u6539\u4e3aLoadBalancer ServiceType\u7684\u51e0\u79cd\u7c7b\u578b\uff1a * ClusterIP: \u7f3a\u7701\u7c7b\u578b\uff0c\u66b4\u9732cluster-internal IP, \u53ea\u80fd\u4ececluster\u5185\u90e8\u8bbf\u95ee * NodePort: \u901a\u8fc7node IP + PORT\u7684\u65b9\u5f0f\u66b4\u9732\uff0c\u4ece\u5916\u90e8\u4ee5 NodeIP : NodePort \u7684\u65b9\u5f0f\u8bbf\u95ee * LoadBalancer: \u901a\u8fc7\u5916\u90e8load balancer\u7684External IP\u66b4\u9732\uff0c\u4ece\u5916\u90e8\u901a\u8fc7External IP\u5730\u5740\u8bbf\u95ee * ExternalName\uff1a \u901a\u8fc7URL\u66b4\u9732 \u7f16\u8f91frontend service, \u5c06type: NodePort\u6539\u4e3atype: LoadBalancer kubectl -n guestbook edit svc frontend snip type: LoadBalancer \u73b0\u5728\u51fa\u73b0\u4e86EXTERNAL-IP mentan@master:~$ kubectl -n guestbook get svc frontend NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE frontend LoadBalancer 172.16.5.43 172.16.2.2 80:31492/TCP 91m \u68c0\u67e5L3out Router\uff0c\u786e\u4fddEXTERNA-IP\u88ab\u6b63\u786e\u7684\u8fdb\u884c\u4e86SNAT IPN_N9K# show ip nat translations Pro Inside global Inside local Outside local Outside global any 10.75.53.112 172.16.2.2 --- --- \u5982\u679c\u4e00\u5207\u6b63\u5e38\uff0c\u6b64\u65f6\u5e94\u8be5\u53ef\u4ee5\u901a\u8fc7EXTERNAL-IP\u8bbf\u95eeGuestbook UI, \u53ef\u4ee5\u8f93\u5165\u4fe1\u606f\u8fdb\u884c\u7559\u8a00 \u68c0\u67e5ACI\u4e0a\u7684\u53d8\u5316\uff1a PBR Policy \u65b0 service external EPG in L3out contract between service external EPG and default external EPG \u65b0 service graph 5. \u5bf9\u5bb9\u5668\u8d44\u6e90\u8fdb\u884c\u4f38\u7f29 \u5c06frontend\u7684pods\u4ece3\u4e2a\u6269\u5c55\u52305\u4e2a kubectl -n guestbook scale deployment frontend --replicas=5 mentan@master:~$ kubectl get deployment --all-namespaces NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE guestbook frontend 5/5 5 5 13h \u4eceACI\u4e2d\u53ef\u4ee5\u6e05\u695a\u7684\u770b\u5230pod\u6570\u91cf\u7684\u53d8\u5316 6. \u5bb9\u5668\u7f51\u7edc\u5b89\u5168\u63a7\u5236 \u68c0\u67e5guestbook\u5e94\u7528\uff0c\u53ef\u4ee5\u6b63\u5e38\u7559\u8a00\u3002\u4e0b\u9762\u6211\u4eec\u5c06\u901a\u8fc7EPG\u7684\u7ec6\u5206\uff0c\u5b9e\u73b0\u7528contract\u6765\u5b9e\u73b0\u5b89\u5168\u63a7\u5236\u3002 \u5728ACI\u4e2d\u521b\u5efa\u4e24\u4e2aEPG, \u5206\u522b\u4e3afrontend, backend\u3002\u6ce8\u610f\u4e0b\u9762EPG\u7684\u53c2\u6570 \u4fee\u6539annotation\uff0c\u5c06EPG\u4eceName space\u7ec6\u5316\u4e3aDeployment\u3002 kubectl edit namespace guestbook \u5220\u9664\u4e0b\u9762\u7684annotation, pods\u5c06\u4eceguestbook EPG\u56de\u5230kube-default EPG annotations: opflex.cisco.com/endpoint-group: '{\"tenant\":\"pov_k8s\",\"app-profile\":\"kubernetes\",\"name\":\"guestbook\"}' \u4f7f\u7528\u4e0b\u9762\u7684\u6307\u4ee4\u4fee\u6539deployment\u7684annotation \u589e\u52a0frontend deployment\u7684annotation kubectl edit deployment frontend -n guestbook \u5728annotation\u4e0b\u589e\u52a0\u4e00\u884c opflex.cisco.com/endpoint-group: '{\"tenant\":\"pov_k8s\",\"app-profile\":\"kubernetes\",\"name\":\"frontend\"}' \u68c0\u67e5ACI frontend EPG, \u53d1\u73b0frontend pods\u5df2\u7ecf\u4ecekube-default\u8f6c\u79fb\u81f3frontend EPG \u589e\u52a0backend deployment\u7684annotation kubectl edit deployment redis-master -n guestbook kubectl edit deployment redis-slave -n guestbook \u5728annotation\u4e0b\u589e\u52a0\u4e00\u884c opflex.cisco.com/endpoint-group: '{\"tenant\":\"pov_k8s\",\"app-profile\":\"kubernetes\",\"name\":\"backend\"}' \u68c0\u67e5ACI backend EPG, \u53d1\u73b0redis-* pods\u5df2\u7ecf\u4ecekube-default\u8f6c\u79fb\u81f3backend EPG \u56de\u5230guestbook \u7f51\u7ad9, \u7f51\u7ad9\u53ef\u4ee5\u6b63\u5e38\u8bbf\u95ee\u4f46\u6b64\u65f6\u5df2\u7ecf\u4e0d\u80fd\u7559\u8a00\u3002 \u4e0b\u9762\u6211\u4eec\u5c06\u5728frontend\u548cbackend EPG\u4e4b\u95f4\u6dfb\u52a0contract\uff0c\u5b9e\u73b0\u524d\u540e\u53f0\u7684\u6b63\u5e38\u901a\u8baf\u3002 \u5728pov_k8s tenant\u521b\u5efacontact guestbook \u5728backend EPG\u5c06\u65b0\u521b\u5efa\u7684guestbook\u6dfb\u52a0provider contract \u5728frontend EPG\u5c06\u65b0\u521b\u5efa\u7684guestbook\u6dfb\u52a0consumer contract \u6700\u7ec8\u7684contract topology\u5982\u4e0b \u518d\u56de\u5230guestbook \u7f51\u7ad9, \u7f51\u7ad9\u53ef\u4ee5\u6b63\u5e38\u8bbf\u95ee\u5e76\u7559\u8a00\u3002","title":"\u90e8\u7f72\u5bb9\u5668\u5e94\u7528"},{"location":"k8s/k8s-guestbook/#guestbook","text":"\u5728\u672c\u6f14\u793a\u7528\uff0c\u4f7f\u7528\u8457\u540d\u7684Guestbook\u6765\u6f14\u793a\u5bb9\u5668\u5e94\u7528\uff0c\u4ece\u4e0b\u9762\u7684\u94fe\u63a5\u4e0b\u8f7dall-in-one guestbook yaml file\uff1a guestbook-all-in-one.yaml","title":"\u90e8\u7f72Guestbook\u5bb9\u5668\u5e94\u7528"},{"location":"k8s/k8s-guestbook/#1-sshmaster-node-guestbook-namespace","text":"SSH login to master node kubectl create namespace guestbook","title":"1. SSH\u767b\u5f55Master Node, \u521b\u5efaguestbook namespace"},{"location":"k8s/k8s-guestbook/#2-all-in-one-guestbook-yaml-file-guestbook-namespace","text":"wget https://raw.githubusercontent.com/kubernetes/examples/master/guestbook/all-in-one/guestbook-all-in-one.yaml kubectl -n guestbook apply -f guestbook-all-in-one.yaml \u68c0\u67e5\u72b6\u6001 root@master:~# kubectl -n guestbook get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES frontend-74b4665db5-b9wqh 1/1 Running 0 4h24m 10.2.0.61 worker1 none none frontend-74b4665db5-j6jhw 1/1 Running 0 4h24m 10.2.0.60 worker1 none none frontend-74b4665db5-pzcmx 1/1 Running 0 4h24m 10.2.0.59 worker1 none none redis-master-6fbbc44567-8zn45 1/1 Running 0 4h24m 10.2.0.56 worker1 none none redis-slave-74ccb764fc-b8mjx 1/1 Running 0 4h24m 10.2.0.57 worker1 none none redis-slave-74ccb764fc-kz8j5 1/1 Running 0 4h24m 10.2.0.58 worker1 none none root@master:~# root@master:~# kubectl -n guestbook get svc","title":"2. \u4e0b\u8f7dall-in-one guestbook yaml file, \u90e8\u7f72\u5728guestbook namespace"},{"location":"k8s/k8s-guestbook/#3-guestbook-namespaceepg","text":"\u624b\u5de5\u521b\u5efa\u65b0\u7684EPG guestbook\uff0c\u9009\u62e9kube-pod-bd\uff0c\u7ee7\u627fkube-default EPG\u7684contract\uff0c\u5173\u8054\u81f3K8s VMM domain \u4f7f\u7528acikubectl\u5de5\u5177\u5c06guestbook namespace\u6dfb\u52a0annotate\u81f3EPG root@master:~# acikubectl set default-eg namespace guestbook -t mykube -a kubernetes -g guestbook Setting default endpoint group: Endpoint Group: Tenant: mykube App profile: kubernetes Endpoint group: guestbook root@master:~# \u6216\u8005\u6807\u51c6\u7684kubectl\u4fee\u6539annotate kubectl annotate namespace guestbook 'opflex.cisco.com/endpoint-group={\"tenant\":\"pov_k8s\",\"app-profile\":\"kubernetes\",\"name\":\"guestbook\"}' \u901a\u8fc7\u4e0b\u9762\u7684\u6307\u4ee4\u68c0\u67e5\u65b0\u7684annotate root@master:~# kubectl describe namespace guestbook Name: guestbook Labels: none Annotations: opflex.cisco.com/endpoint-group: {\"tenant\":\"mykube\",\"app-profile\":\"kubernetes\",\"name\":\"guestbook\"} Status: Active \u4eceACI\u8fdb\u884c\u68c0\u67e5\uff0c\u53d1\u73b0guestbook\u76f8\u5173\u7684pod\u4ecekube-default\u8f6c\u79fb\u5230\u4e86\u65b0\u5efa\u7684guestbook EPG","title":"3. \u5c06guestbook namespace\u5173\u8054\u81f3\u5355\u72ec\u7684EPG"},{"location":"k8s/k8s-guestbook/#4-guestbook","text":"\u5148\u68c0\u67e5ACI CNI\u81ea\u52a8\u521b\u5efa\u7684L4-7 service graph\u914d\u7f6e In common tenant, \u81ea\u52a8\u521b\u5efaL4-7 device service BD\u7528\u4e8ePBR service graph template \u4e0b\u9762\u5f00\u59cb\u8fdb\u884c\u914d\u7f6e\u5bf9\u5916\u66b4\u9732guestbook service \u5c06frontend ServiceType\u7531\u7f3a\u7701\u7684NodePort\u6539\u4e3aLoadBalancer ServiceType\u7684\u51e0\u79cd\u7c7b\u578b\uff1a * ClusterIP: \u7f3a\u7701\u7c7b\u578b\uff0c\u66b4\u9732cluster-internal IP, \u53ea\u80fd\u4ececluster\u5185\u90e8\u8bbf\u95ee * NodePort: \u901a\u8fc7node IP + PORT\u7684\u65b9\u5f0f\u66b4\u9732\uff0c\u4ece\u5916\u90e8\u4ee5 NodeIP : NodePort \u7684\u65b9\u5f0f\u8bbf\u95ee * LoadBalancer: \u901a\u8fc7\u5916\u90e8load balancer\u7684External IP\u66b4\u9732\uff0c\u4ece\u5916\u90e8\u901a\u8fc7External IP\u5730\u5740\u8bbf\u95ee * ExternalName\uff1a \u901a\u8fc7URL\u66b4\u9732 \u7f16\u8f91frontend service, \u5c06type: NodePort\u6539\u4e3atype: LoadBalancer kubectl -n guestbook edit svc frontend snip type: LoadBalancer \u73b0\u5728\u51fa\u73b0\u4e86EXTERNAL-IP mentan@master:~$ kubectl -n guestbook get svc frontend NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE frontend LoadBalancer 172.16.5.43 172.16.2.2 80:31492/TCP 91m \u68c0\u67e5L3out Router\uff0c\u786e\u4fddEXTERNA-IP\u88ab\u6b63\u786e\u7684\u8fdb\u884c\u4e86SNAT IPN_N9K# show ip nat translations Pro Inside global Inside local Outside local Outside global any 10.75.53.112 172.16.2.2 --- --- \u5982\u679c\u4e00\u5207\u6b63\u5e38\uff0c\u6b64\u65f6\u5e94\u8be5\u53ef\u4ee5\u901a\u8fc7EXTERNAL-IP\u8bbf\u95eeGuestbook UI, \u53ef\u4ee5\u8f93\u5165\u4fe1\u606f\u8fdb\u884c\u7559\u8a00 \u68c0\u67e5ACI\u4e0a\u7684\u53d8\u5316\uff1a PBR Policy \u65b0 service external EPG in L3out contract between service external EPG and default external EPG \u65b0 service graph","title":"4. \u5bf9\u5916\u66b4\u9732guestbook\u670d\u52a1"},{"location":"k8s/k8s-guestbook/#5","text":"\u5c06frontend\u7684pods\u4ece3\u4e2a\u6269\u5c55\u52305\u4e2a kubectl -n guestbook scale deployment frontend --replicas=5 mentan@master:~$ kubectl get deployment --all-namespaces NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE guestbook frontend 5/5 5 5 13h \u4eceACI\u4e2d\u53ef\u4ee5\u6e05\u695a\u7684\u770b\u5230pod\u6570\u91cf\u7684\u53d8\u5316","title":"5. \u5bf9\u5bb9\u5668\u8d44\u6e90\u8fdb\u884c\u4f38\u7f29"},{"location":"k8s/k8s-guestbook/#6","text":"\u68c0\u67e5guestbook\u5e94\u7528\uff0c\u53ef\u4ee5\u6b63\u5e38\u7559\u8a00\u3002\u4e0b\u9762\u6211\u4eec\u5c06\u901a\u8fc7EPG\u7684\u7ec6\u5206\uff0c\u5b9e\u73b0\u7528contract\u6765\u5b9e\u73b0\u5b89\u5168\u63a7\u5236\u3002 \u5728ACI\u4e2d\u521b\u5efa\u4e24\u4e2aEPG, \u5206\u522b\u4e3afrontend, backend\u3002\u6ce8\u610f\u4e0b\u9762EPG\u7684\u53c2\u6570 \u4fee\u6539annotation\uff0c\u5c06EPG\u4eceName space\u7ec6\u5316\u4e3aDeployment\u3002 kubectl edit namespace guestbook \u5220\u9664\u4e0b\u9762\u7684annotation, pods\u5c06\u4eceguestbook EPG\u56de\u5230kube-default EPG annotations: opflex.cisco.com/endpoint-group: '{\"tenant\":\"pov_k8s\",\"app-profile\":\"kubernetes\",\"name\":\"guestbook\"}' \u4f7f\u7528\u4e0b\u9762\u7684\u6307\u4ee4\u4fee\u6539deployment\u7684annotation \u589e\u52a0frontend deployment\u7684annotation kubectl edit deployment frontend -n guestbook \u5728annotation\u4e0b\u589e\u52a0\u4e00\u884c opflex.cisco.com/endpoint-group: '{\"tenant\":\"pov_k8s\",\"app-profile\":\"kubernetes\",\"name\":\"frontend\"}' \u68c0\u67e5ACI frontend EPG, \u53d1\u73b0frontend pods\u5df2\u7ecf\u4ecekube-default\u8f6c\u79fb\u81f3frontend EPG \u589e\u52a0backend deployment\u7684annotation kubectl edit deployment redis-master -n guestbook kubectl edit deployment redis-slave -n guestbook \u5728annotation\u4e0b\u589e\u52a0\u4e00\u884c opflex.cisco.com/endpoint-group: '{\"tenant\":\"pov_k8s\",\"app-profile\":\"kubernetes\",\"name\":\"backend\"}' \u68c0\u67e5ACI backend EPG, \u53d1\u73b0redis-* pods\u5df2\u7ecf\u4ecekube-default\u8f6c\u79fb\u81f3backend EPG \u56de\u5230guestbook \u7f51\u7ad9, \u7f51\u7ad9\u53ef\u4ee5\u6b63\u5e38\u8bbf\u95ee\u4f46\u6b64\u65f6\u5df2\u7ecf\u4e0d\u80fd\u7559\u8a00\u3002 \u4e0b\u9762\u6211\u4eec\u5c06\u5728frontend\u548cbackend EPG\u4e4b\u95f4\u6dfb\u52a0contract\uff0c\u5b9e\u73b0\u524d\u540e\u53f0\u7684\u6b63\u5e38\u901a\u8baf\u3002 \u5728pov_k8s tenant\u521b\u5efacontact guestbook \u5728backend EPG\u5c06\u65b0\u521b\u5efa\u7684guestbook\u6dfb\u52a0provider contract \u5728frontend EPG\u5c06\u65b0\u521b\u5efa\u7684guestbook\u6dfb\u52a0consumer contract \u6700\u7ec8\u7684contract topology\u5982\u4e0b \u518d\u56de\u5230guestbook \u7f51\u7ad9, \u7f51\u7ad9\u53ef\u4ee5\u6b63\u5e38\u8bbf\u95ee\u5e76\u7559\u8a00\u3002","title":"6. \u5bb9\u5668\u7f51\u7edc\u5b89\u5168\u63a7\u5236"},{"location":"k8s/k8s-index/","text":"\u80cc\u666f\u4fe1\u606f \u76ee\u524dACI\u652f\u6301\u4ee5\u4e0b\u4e09\u79cdK8s\u53d1\u884c\u7248\u672c\uff0c Openshift\u548cDocker EE\u8981\u6c42\u4f7f\u7528RHEL\uff0c Canonical K8s\u652f\u6301Ubuntu\u3002RHEL\u9700\u8981\u53d6\u5f97\u7ea2\u5e3d\u7684\u8f6f\u4ef6\u8ba2\u9605\u670d\u52a1\uff0cCanonical K8s\u53ef\u4ee5\u4f7f\u7528\u5f00\u653e\u7684Ubuntu\uff0c\u5728\u672c\u5c55\u793a\u4e2d\u6211\u4eec\u4f7f\u7528Canonical K8s Canonical Kubernetes on Ubuntu 18.04 Red Hat Openshift on RHEL 7 Docker Enterprise on RHEL 7 \u8bbf\u95ee\u4e0b\u9762\u7684\u7f51\u7ad9\u4e86\u89e3\u6700\u65b0\u7684ACI CNI\u652f\u6301\u4fe1\u606f\uff1a ACI Virtualization Compatibility Matrix \u5728ACI\u4e0a\u90e8\u7f72ACI CNI\u7684\u57fa\u672c\u6d41\u7a0b\u5982\u4e0b:","title":"\u80cc\u666f\u4fe1\u606f"},{"location":"k8s/k8s-index/#_1","text":"\u76ee\u524dACI\u652f\u6301\u4ee5\u4e0b\u4e09\u79cdK8s\u53d1\u884c\u7248\u672c\uff0c Openshift\u548cDocker EE\u8981\u6c42\u4f7f\u7528RHEL\uff0c Canonical K8s\u652f\u6301Ubuntu\u3002RHEL\u9700\u8981\u53d6\u5f97\u7ea2\u5e3d\u7684\u8f6f\u4ef6\u8ba2\u9605\u670d\u52a1\uff0cCanonical K8s\u53ef\u4ee5\u4f7f\u7528\u5f00\u653e\u7684Ubuntu\uff0c\u5728\u672c\u5c55\u793a\u4e2d\u6211\u4eec\u4f7f\u7528Canonical K8s Canonical Kubernetes on Ubuntu 18.04 Red Hat Openshift on RHEL 7 Docker Enterprise on RHEL 7 \u8bbf\u95ee\u4e0b\u9762\u7684\u7f51\u7ad9\u4e86\u89e3\u6700\u65b0\u7684ACI CNI\u652f\u6301\u4fe1\u606f\uff1a ACI Virtualization Compatibility Matrix \u5728ACI\u4e0a\u90e8\u7f72ACI CNI\u7684\u57fa\u672c\u6d41\u7a0b\u5982\u4e0b:","title":"\u80cc\u666f\u4fe1\u606f"},{"location":"k8s/k8s-installation/","text":"K8s\u5b89\u88c5 1. Ubuntu VM\u5b89\u88c5 ubuntu vm \u53c2\u6570 Ubuntu 16.04.6 ubuntu-16.04.6-server-amd64.iso Master/Worker: 2 GB RAM 2 Core of CPU 16G HDD VM\u521b\u5efa\u4e24\u4e2aNIC\uff0c \u4e00\u4e2a\u8fde\u63a5OOB\u7f51\u7edc\uff0c \u53e6\u4e00\u4e2a\u5907\u7528 VM\u5b89\u88c5ubuntu\u64cd\u4f5c\u7cfb\u7edf\uff0c\u5b8c\u6210\u540e\u521b\u5efasnapshot\uff0c\u5e76clone\u4e24\u4e2a\u4f5c\u4e3aworker node. 2. \u4e0b\u8f7dacc-provision\u5de5\u5177\uff0c\u751f\u6210ACI\u914d\u7f6e \u4ece\u4e0b\u9762\u7684\u94fe\u63a5\u4e0b\u8f7dacc-provison\u5de5\u5177 APIC OpenStack and Container Plugins \u5bf9\u5e94Ubuntu\u7684\u6587\u4ef6\u683c\u5f0f\u4e3a dist-debs-4.1.1.2.tar.gz \u901a\u8fc7scp\u5c06\u4e0b\u8f7d\u7684\u6587\u4ef6\u4e0a\u4f20\u81f3master VM scp dist-debs-4.1.1.2.tar.gz mentan@10.75.53.88:/home/mentan \u89e3\u538b\u7f29\u5f97\u5230.deb\u6587\u4ef6 tar \u2013xzf dist-debs-4.1.1.2.tar.gz acc-provision_4.1.1.2-13_amd64.deb 3. \u5b89\u88c5ACI CNI Plugin sudo -i cp /home/mentan/acc-provision_4.1.1.2-13_amd64.deb . apt update apt -f install ./acc-provision_4.1.1.2-13_amd64.deb 4. \u751f\u6210ACI CNI Plugin\u914d\u7f6e\u6587\u4ef6 acc-provision --flavor=kubernetes-1.13 --sample acc-config.yaml 5. \u7f16\u8f91ACI CNI Plugin\u914d\u7f6e\u6587\u4ef6 (\u6ce8\u610f\u4fee\u6539\u90e8\u5206\u7684(!)\u6807\u8bb0\u8bf4\u660e) Note: yaml\u6587\u4ef6\u4e00\u5b9a\u8981\u6ce8\u610f\u7f29\u8fdb\uff0c\u5426\u5219\u4f1a\u62a5\u9519\uff01 # aci_config: system_id: pov_k8s # Every opflex cluster must have a distict ID apic_hosts: # List of APIC hosts to connect for APIC API - 10.75.53.121 vmm_domain: # Kubernetes container domain configuration encap_type: vxlan # Encap mode: vxlan or vlan mcast_range: # Every opflex VMM must use a distinct range start: 225.20.1.1 end: 225.20.255.255 nested_inside: # Include if nested inside a VMM; # required for CloudFoundry # supported for Kubernetes type: vmware # Specify the VMM vendor (supported: vmware) name: POV-UCS-HX # Specify the name of the VMM domain # The following resources must already exist on the APIC. # They are used, but not created, by the provisioning tool. aep: POV-UCS-HX # The AEP for ports/VPCs used by this cluster vrf: # This VRF used to create all kubernetes EPs name: default tenant: common # This can be system-id or common l3out: name: POV-COMMON-L3OUT # Used to provision external IPs external_networks: - EXT # Used for external contracts #custom_epgs: # List of additional endpoint-group names # - custom_group1 # to configure for use with annotations # - custom_group2 #isolation_segments: # List of names of isolation segments # - name: iso-seg-1 # and the subnets to use (CloudFoundry only) # subnet: 10.11.0.1/24 # - name: iso-seg-2 # subnet: 10.11.1.1/24 # # Networks used by ACI containers # net_config: node_subnet: 172.16.0.1/24 # Subnet to use for nodes pod_subnet: 172.16.1.1/24 # Subnet to use for Kubernetes # Pods/CloudFoundry containers #vip_subnet: 10.3.0.1/16 # Subnet to use for virtual IPs # (CloudFoundry only) extern_dynamic: 172.16.2.1/24 # Subnet to use for dynamic external IPs extern_static: 172.16.3.1/24 # Subnet to use for static external IPs node_svc_subnet: 172.16.4.1/24 # Subnet to use for service graph kubeapi_vlan: 4001 # The VLAN used by the physdom for nodes # (Kubernetes only) service_vlan: 4003 # The VLAN used by LoadBalancer services infra_vlan: 4093 # The VLAN used by ACI infra #interface_mtu: 1600 # min = 1280 for ipv6, max = 8900 for VXLAN # # Configuration for container registry # Update if a custom container registry has been setup # registry: image_prefix: noiro # e.g: registry.example.com/noiro # image_pull_secret: secret_name # (if needed) 6. \u751f\u6210ACI\u914d\u7f6e\u548c\u65b0\u7684CNI plugin\u914d\u7f6e\u6587\u4ef6 acc-provision --flavor=kubernetes-1.13 -a -u mentan -p xxxxxx -c acc-config.yaml -o aci-cni-config.yaml \u5b8c\u6210\u540e\uff0cACI\u4e0a\u81ea\u52a8\u751f\u6210\u4e0b\u5217\u914d\u7f6e\uff1a tenant/EPG contract BD physical domain vlan pool AAEP\u53d8\u66f4 K8S VMM 7. VM \u8bbe\u7f6e (\u5305\u62ecMaster\u548cworker nodes) \u4fee\u6539VM Hostname vi /etc/hostname vi /etc/hosts restart vm \u8fdb\u5165VCenter\uff0c\u5c06VM\u7684\u7b2c\u4e8cNIC\u7684port-group\u6539\u4e3ayaml\u6587\u4ef6\u4e2dsystem_id\u5bf9\u5e94\u7684port-group \u8bbe\u7f6e NIC ip address, sudo vi /etc/network/interfaces #OOB Connection auto ens160 iface ens160 inet static address 10.75.53.88 netmask 255.255.255.0 up route add -net 10.0.0.0/8 gw 10.75.53.1 dns-nameservers 64.104.123.245 # Interface connected to ACI auto ens192 iface ens192 inet manual mtu 9000 # ACI Infra VLAN auto ens192.4093 iface ens192.4093 inet dhcp mtu 9000 up route add -net 224.0.0.0/4 dev ens192.4093 vlan-raw-device ens192 # Node Vlan (kubeapi_vlan) auto ens192.4001 iface ens192.4001 inet static address 172.16.0.11 netmask 255.255.255.0 mtu 9000 vlan-raw-device ens192 up route add default gw 172.16.0.1 dns-nameservers 64.104.123.245 \u8bbe\u7f6eDHCP Client, \u5c06XX:XX:XX:XX:XX:XX\u4fee\u6539\u4e3aens192\u7684MAC\u5730\u5740 vi /etc/dhcp/dhclient.conf send dhcp-client-identifier 01:XX:XX:XX:XX:XX:XX; request subnet-mask, domain-name, domain-name-servers, host-name; send host-name = gethostname(); option rfc3442-classless-static-routes code 121 = array of unsigned integer 8; option ms-classless-static-routes code 249 = array of unsigned integer 8; option wpad code 252 = string; also request rfc3442-classless-static-routes; also request ms-classless-static-routes; also request static-routes; also request wpad; also request ntp-servers; timeout 10; \u4fee\u6539\u5b8c\u6bd5\u540e\u91cd\u542f\u7f51\u7edc\u670d\u52a1\u6216\u91cd\u542fVM systemctl restart networking \u68c0\u67e5\u7f51\u7edc\u8bbe\u7f6e mentan@master:~$ ip route |grep ens192 default via 172.16.100.1 dev ens192.4001 10.0.0.0/16 dev ens192.4093 proto kernel scope link src 10.0.152.64 172.16.100.0/24 dev ens192.4001 proto kernel scope link src 172.16.100.11 224.0.0.0/4 dev ens192.4093 scope link mentan@master:~$ mentan@master:~$ ip addr |grep ens192 |grep inet inet 10.0.152.64/16 brd 10.0.255.255 scope global ens192.4093 inet 172.16.100.11/24 brd 172.16.100.255 scope global ens192.4001 mentan@master:~$ mentan@master:~$ ip link |grep ens192 |grep mtu 3: ens192: BROADCAST,MULTICAST,UP,LOWER_UP mtu 9000 qdisc mq state UP mode DEFAULT group default qlen 1000 4: ens192.4093@ens192: BROADCAST,MULTICAST,UP,LOWER_UP mtu 9000 qdisc noqueue state UP mode DEFAULT group default qlen 1000 5: ens192.4001@ens192: BROADCAST,MULTICAST,UP,LOWER_UP mtu 9000 qdisc noqueue state UP mode DEFAULT group default qlen 1000 mentan@master:~$ \u6b64\u65f6\uff0cnode EPG\u4e2d\u5e94\u8be5\u53ef\u4ee5\u770b\u5230node learned\uff0cnode\u4e5f\u5e94\u8be5\u53ef\u4ee5ping\u901a\u7f51\u5173 8. \u5b89\u88c5Docker (all nodes) apt install docker.io docker pull busybox 9. \u5b89\u88c5Kubeadm (all nodes) \u6dfb\u52a0k8s repository: apt-get update apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat EOF /etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update \u5b89\u88c5kubectl, kubeadm, kubernetes-cni (1.13.5) apt install -y kubeadm=1.13.5-00 kubelet=1.13.5-00 kubectl=1.13.5-00 kubernetes-cni=0.7.5-00 10. Disable SWAP (all nodes) swapoff -a vi /etc/fstab \u6ce8\u91ca\u4e0b\u9762\u7684swap\u6761\u76ee\uff0c #/dev/mapper/master--vg-swap_1 none swap sw 0 0 11. \u521d\u59cb\u5316master node kubeadm init --pod-network-cidr=172.16.1.1/24 --service-cidr=172.16.5.1/24 snip Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join 172.16.0.11:6443 --token pvbozs.zbr8n5fyv061asja --discovery-token-ca-cert-hash sha256:6afb92a946945451d60378d2441ca21ca819681a68afe22e7414f5128bbc9edf \u4ee5\u666e\u901a\u7528\u6237\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\uff1a mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 12. worker node\u52a0\u5165\u96c6\u7fa4 \u5728\u6240\u6709worker node\u4e0a\u4ee5root\u7528\u6237\u6267\u884c\u4ee5\u4e0b\u6307\u4ee4\uff1a kubeadm join 172.16.100.11:6443 --token ofbxss.x77763eisajl09dc --discovery-token-ca-cert-hash sha256:ad732e4935337bfce65d96cf2004247ec23b222416881487c129ee52a753bee1 \u5728master node\u4e0a\u68c0\u67e5cluster\u72b6\u6001\uff0c \u72b6\u6001NotReady\u662f\u6b63\u5e38\u7684\uff0c\u56e0\u4e3aCNI\u8fd8\u672a\u5b89\u88c5 mentan@master:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION master NotReady master 19m v1.13.5 worker1 NotReady none 10m v1.13.5 13. \u90e8\u7f72K8S Dashboard \u672c\u5904\u53c2\u8003\u4e86\u4e0b\u9762\u7684\u94fe\u63a5\uff1a link1 \u7f3a\u7701K8s\u4e0d\u4f1a\u90e8\u7f72Dashboard\uff0c\u53ef\u4ee5\u4f7f\u7528\u4e0b\u9762\u7684\u547d\u4ee4\u90e8\u7f72 kubectl apply -f http://mirror.faasx.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml \u7f16\u8f91dashboard service\uff0c\u5c06type\u4eceNodePort\u6539\u4e3aLoadBalancer\uff0c\u5b9e\u73b0Dashboard\u7684\u5916\u7f51\u8bbf\u95ee kubectl edit service kubernetes-dashboard -n kube-system \u901a\u8fc7\u66b4\u9732\u7684external-ip\u8bbf\u95eeDashboard\uff0c\u51fa\u73b0\u4e0b\u9762\u7684\u767b\u5f55\u7a97\u53e3 \u901a\u8fc7\u4e0a\u9762\u53c2\u8003\u94fe\u63a5\u4e2d\u7684\u65b9\u6cd5\u751f\u6210admin token\uff0c\u8f93\u5165\u540e\u8fdb\u5165Dashboard","title":"K8s\u5b89\u88c5"},{"location":"k8s/k8s-installation/#k8s","text":"","title":"K8s\u5b89\u88c5"},{"location":"k8s/k8s-installation/#1-ubuntu-vm","text":"ubuntu vm \u53c2\u6570 Ubuntu 16.04.6 ubuntu-16.04.6-server-amd64.iso Master/Worker: 2 GB RAM 2 Core of CPU 16G HDD VM\u521b\u5efa\u4e24\u4e2aNIC\uff0c \u4e00\u4e2a\u8fde\u63a5OOB\u7f51\u7edc\uff0c \u53e6\u4e00\u4e2a\u5907\u7528 VM\u5b89\u88c5ubuntu\u64cd\u4f5c\u7cfb\u7edf\uff0c\u5b8c\u6210\u540e\u521b\u5efasnapshot\uff0c\u5e76clone\u4e24\u4e2a\u4f5c\u4e3aworker node.","title":"1. Ubuntu VM\u5b89\u88c5"},{"location":"k8s/k8s-installation/#2-acc-provisionaci","text":"\u4ece\u4e0b\u9762\u7684\u94fe\u63a5\u4e0b\u8f7dacc-provison\u5de5\u5177 APIC OpenStack and Container Plugins \u5bf9\u5e94Ubuntu\u7684\u6587\u4ef6\u683c\u5f0f\u4e3a dist-debs-4.1.1.2.tar.gz \u901a\u8fc7scp\u5c06\u4e0b\u8f7d\u7684\u6587\u4ef6\u4e0a\u4f20\u81f3master VM scp dist-debs-4.1.1.2.tar.gz mentan@10.75.53.88:/home/mentan \u89e3\u538b\u7f29\u5f97\u5230.deb\u6587\u4ef6 tar \u2013xzf dist-debs-4.1.1.2.tar.gz acc-provision_4.1.1.2-13_amd64.deb","title":"2. \u4e0b\u8f7dacc-provision\u5de5\u5177\uff0c\u751f\u6210ACI\u914d\u7f6e"},{"location":"k8s/k8s-installation/#3-aci-cni-plugin","text":"sudo -i cp /home/mentan/acc-provision_4.1.1.2-13_amd64.deb . apt update apt -f install ./acc-provision_4.1.1.2-13_amd64.deb","title":"3. \u5b89\u88c5ACI CNI Plugin"},{"location":"k8s/k8s-installation/#4-aci-cni-plugin","text":"acc-provision --flavor=kubernetes-1.13 --sample acc-config.yaml","title":"4. \u751f\u6210ACI CNI Plugin\u914d\u7f6e\u6587\u4ef6"},{"location":"k8s/k8s-installation/#5-aci-cni-plugin","text":"Note: yaml\u6587\u4ef6\u4e00\u5b9a\u8981\u6ce8\u610f\u7f29\u8fdb\uff0c\u5426\u5219\u4f1a\u62a5\u9519\uff01 # aci_config: system_id: pov_k8s # Every opflex cluster must have a distict ID apic_hosts: # List of APIC hosts to connect for APIC API - 10.75.53.121 vmm_domain: # Kubernetes container domain configuration encap_type: vxlan # Encap mode: vxlan or vlan mcast_range: # Every opflex VMM must use a distinct range start: 225.20.1.1 end: 225.20.255.255 nested_inside: # Include if nested inside a VMM; # required for CloudFoundry # supported for Kubernetes type: vmware # Specify the VMM vendor (supported: vmware) name: POV-UCS-HX # Specify the name of the VMM domain # The following resources must already exist on the APIC. # They are used, but not created, by the provisioning tool. aep: POV-UCS-HX # The AEP for ports/VPCs used by this cluster vrf: # This VRF used to create all kubernetes EPs name: default tenant: common # This can be system-id or common l3out: name: POV-COMMON-L3OUT # Used to provision external IPs external_networks: - EXT # Used for external contracts #custom_epgs: # List of additional endpoint-group names # - custom_group1 # to configure for use with annotations # - custom_group2 #isolation_segments: # List of names of isolation segments # - name: iso-seg-1 # and the subnets to use (CloudFoundry only) # subnet: 10.11.0.1/24 # - name: iso-seg-2 # subnet: 10.11.1.1/24 # # Networks used by ACI containers # net_config: node_subnet: 172.16.0.1/24 # Subnet to use for nodes pod_subnet: 172.16.1.1/24 # Subnet to use for Kubernetes # Pods/CloudFoundry containers #vip_subnet: 10.3.0.1/16 # Subnet to use for virtual IPs # (CloudFoundry only) extern_dynamic: 172.16.2.1/24 # Subnet to use for dynamic external IPs extern_static: 172.16.3.1/24 # Subnet to use for static external IPs node_svc_subnet: 172.16.4.1/24 # Subnet to use for service graph kubeapi_vlan: 4001 # The VLAN used by the physdom for nodes # (Kubernetes only) service_vlan: 4003 # The VLAN used by LoadBalancer services infra_vlan: 4093 # The VLAN used by ACI infra #interface_mtu: 1600 # min = 1280 for ipv6, max = 8900 for VXLAN # # Configuration for container registry # Update if a custom container registry has been setup # registry: image_prefix: noiro # e.g: registry.example.com/noiro # image_pull_secret: secret_name # (if needed)","title":"5. \u7f16\u8f91ACI CNI Plugin\u914d\u7f6e\u6587\u4ef6 (\u6ce8\u610f\u4fee\u6539\u90e8\u5206\u7684(!)\u6807\u8bb0\u8bf4\u660e)"},{"location":"k8s/k8s-installation/#6-acicni-plugin","text":"acc-provision --flavor=kubernetes-1.13 -a -u mentan -p xxxxxx -c acc-config.yaml -o aci-cni-config.yaml \u5b8c\u6210\u540e\uff0cACI\u4e0a\u81ea\u52a8\u751f\u6210\u4e0b\u5217\u914d\u7f6e\uff1a tenant/EPG contract BD physical domain vlan pool AAEP\u53d8\u66f4 K8S VMM","title":"6. \u751f\u6210ACI\u914d\u7f6e\u548c\u65b0\u7684CNI plugin\u914d\u7f6e\u6587\u4ef6"},{"location":"k8s/k8s-installation/#7-vm-masterworker-nodes","text":"\u4fee\u6539VM Hostname vi /etc/hostname vi /etc/hosts restart vm \u8fdb\u5165VCenter\uff0c\u5c06VM\u7684\u7b2c\u4e8cNIC\u7684port-group\u6539\u4e3ayaml\u6587\u4ef6\u4e2dsystem_id\u5bf9\u5e94\u7684port-group \u8bbe\u7f6e NIC ip address, sudo vi /etc/network/interfaces #OOB Connection auto ens160 iface ens160 inet static address 10.75.53.88 netmask 255.255.255.0 up route add -net 10.0.0.0/8 gw 10.75.53.1 dns-nameservers 64.104.123.245 # Interface connected to ACI auto ens192 iface ens192 inet manual mtu 9000 # ACI Infra VLAN auto ens192.4093 iface ens192.4093 inet dhcp mtu 9000 up route add -net 224.0.0.0/4 dev ens192.4093 vlan-raw-device ens192 # Node Vlan (kubeapi_vlan) auto ens192.4001 iface ens192.4001 inet static address 172.16.0.11 netmask 255.255.255.0 mtu 9000 vlan-raw-device ens192 up route add default gw 172.16.0.1 dns-nameservers 64.104.123.245 \u8bbe\u7f6eDHCP Client, \u5c06XX:XX:XX:XX:XX:XX\u4fee\u6539\u4e3aens192\u7684MAC\u5730\u5740 vi /etc/dhcp/dhclient.conf send dhcp-client-identifier 01:XX:XX:XX:XX:XX:XX; request subnet-mask, domain-name, domain-name-servers, host-name; send host-name = gethostname(); option rfc3442-classless-static-routes code 121 = array of unsigned integer 8; option ms-classless-static-routes code 249 = array of unsigned integer 8; option wpad code 252 = string; also request rfc3442-classless-static-routes; also request ms-classless-static-routes; also request static-routes; also request wpad; also request ntp-servers; timeout 10; \u4fee\u6539\u5b8c\u6bd5\u540e\u91cd\u542f\u7f51\u7edc\u670d\u52a1\u6216\u91cd\u542fVM systemctl restart networking \u68c0\u67e5\u7f51\u7edc\u8bbe\u7f6e mentan@master:~$ ip route |grep ens192 default via 172.16.100.1 dev ens192.4001 10.0.0.0/16 dev ens192.4093 proto kernel scope link src 10.0.152.64 172.16.100.0/24 dev ens192.4001 proto kernel scope link src 172.16.100.11 224.0.0.0/4 dev ens192.4093 scope link mentan@master:~$ mentan@master:~$ ip addr |grep ens192 |grep inet inet 10.0.152.64/16 brd 10.0.255.255 scope global ens192.4093 inet 172.16.100.11/24 brd 172.16.100.255 scope global ens192.4001 mentan@master:~$ mentan@master:~$ ip link |grep ens192 |grep mtu 3: ens192: BROADCAST,MULTICAST,UP,LOWER_UP mtu 9000 qdisc mq state UP mode DEFAULT group default qlen 1000 4: ens192.4093@ens192: BROADCAST,MULTICAST,UP,LOWER_UP mtu 9000 qdisc noqueue state UP mode DEFAULT group default qlen 1000 5: ens192.4001@ens192: BROADCAST,MULTICAST,UP,LOWER_UP mtu 9000 qdisc noqueue state UP mode DEFAULT group default qlen 1000 mentan@master:~$ \u6b64\u65f6\uff0cnode EPG\u4e2d\u5e94\u8be5\u53ef\u4ee5\u770b\u5230node learned\uff0cnode\u4e5f\u5e94\u8be5\u53ef\u4ee5ping\u901a\u7f51\u5173","title":"7. VM \u8bbe\u7f6e (\u5305\u62ecMaster\u548cworker nodes)"},{"location":"k8s/k8s-installation/#8-docker-all-nodes","text":"apt install docker.io docker pull busybox","title":"8. \u5b89\u88c5Docker (all nodes)"},{"location":"k8s/k8s-installation/#9-kubeadm-all-nodes","text":"\u6dfb\u52a0k8s repository: apt-get update apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat EOF /etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update \u5b89\u88c5kubectl, kubeadm, kubernetes-cni (1.13.5) apt install -y kubeadm=1.13.5-00 kubelet=1.13.5-00 kubectl=1.13.5-00 kubernetes-cni=0.7.5-00","title":"9. \u5b89\u88c5Kubeadm (all nodes)"},{"location":"k8s/k8s-installation/#10-disable-swap-all-nodes","text":"swapoff -a vi /etc/fstab \u6ce8\u91ca\u4e0b\u9762\u7684swap\u6761\u76ee\uff0c #/dev/mapper/master--vg-swap_1 none swap sw 0 0","title":"10. Disable SWAP (all nodes)"},{"location":"k8s/k8s-installation/#11-master-node","text":"kubeadm init --pod-network-cidr=172.16.1.1/24 --service-cidr=172.16.5.1/24 snip Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join 172.16.0.11:6443 --token pvbozs.zbr8n5fyv061asja --discovery-token-ca-cert-hash sha256:6afb92a946945451d60378d2441ca21ca819681a68afe22e7414f5128bbc9edf \u4ee5\u666e\u901a\u7528\u6237\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\uff1a mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config","title":"11. \u521d\u59cb\u5316master node"},{"location":"k8s/k8s-installation/#12-worker-node","text":"\u5728\u6240\u6709worker node\u4e0a\u4ee5root\u7528\u6237\u6267\u884c\u4ee5\u4e0b\u6307\u4ee4\uff1a kubeadm join 172.16.100.11:6443 --token ofbxss.x77763eisajl09dc --discovery-token-ca-cert-hash sha256:ad732e4935337bfce65d96cf2004247ec23b222416881487c129ee52a753bee1 \u5728master node\u4e0a\u68c0\u67e5cluster\u72b6\u6001\uff0c \u72b6\u6001NotReady\u662f\u6b63\u5e38\u7684\uff0c\u56e0\u4e3aCNI\u8fd8\u672a\u5b89\u88c5 mentan@master:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION master NotReady master 19m v1.13.5 worker1 NotReady none 10m v1.13.5","title":"12. worker node\u52a0\u5165\u96c6\u7fa4"},{"location":"k8s/k8s-installation/#13-k8s-dashboard","text":"\u672c\u5904\u53c2\u8003\u4e86\u4e0b\u9762\u7684\u94fe\u63a5\uff1a link1 \u7f3a\u7701K8s\u4e0d\u4f1a\u90e8\u7f72Dashboard\uff0c\u53ef\u4ee5\u4f7f\u7528\u4e0b\u9762\u7684\u547d\u4ee4\u90e8\u7f72 kubectl apply -f http://mirror.faasx.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml \u7f16\u8f91dashboard service\uff0c\u5c06type\u4eceNodePort\u6539\u4e3aLoadBalancer\uff0c\u5b9e\u73b0Dashboard\u7684\u5916\u7f51\u8bbf\u95ee kubectl edit service kubernetes-dashboard -n kube-system \u901a\u8fc7\u66b4\u9732\u7684external-ip\u8bbf\u95eeDashboard\uff0c\u51fa\u73b0\u4e0b\u9762\u7684\u767b\u5f55\u7a97\u53e3 \u901a\u8fc7\u4e0a\u9762\u53c2\u8003\u94fe\u63a5\u4e2d\u7684\u65b9\u6cd5\u751f\u6210admin token\uff0c\u8f93\u5165\u540e\u8fdb\u5165Dashboard","title":"13. \u90e8\u7f72K8S Dashboard"},{"location":"k8s/k8s-prerequisite/","text":"\u96c6\u6210\u524d\u51c6\u5907 Prerequisite before Integration K8s\u652f\u6301\u8fd0\u884c\u4e0eBareMetal\u670d\u52a1\u5668\u548cVM on Hypervisor\uff0c\u5728\u672c\u6f14\u793a\u4e2d\uff0c\u91c7\u7528K8s on VM\u7684\u90e8\u7f72\u65b9\u5f0f\u3002 K8s\u7f51\u7edc\u5730\u5740\u89c4\u5212 NTP: 10.66.141.50, 10.66.141.51 DNS: 64.104.123.245 Node OOB IP: Master: 10.75.53.81 Worker1: 10.75.53.82 Worker2: 10.75.53.83 kubeapi_vlan: 4001 (\u7528\u4e8enode\u7684physical domain) service_vlan: 4003 \uff08\u7528\u4e8eservice\u7684loadbalancer\uff09 Infra_vlan: 4093 node_subnet: 172.16.0.1/24 pod_subnet: 172.16.1.1/24 extern_dynamic: 172.16.2.1/24 extern_static: 172.16.3.1/24 node_svc_subnet: 172.16.4.1/24 cluster_svc_subnet: 172.16.5.1/24 mcast_fabric: 225.1.1.1 mcast_range: start: 225.100.1.1 end: 225.100.1.255 ACI \u73af\u5883\u51c6\u5907 ACI prerequisite 1. ACI VMM\u914d\u7f6e\uff0cACI\u5e94\u8be5\u6b63\u5e38\u548cVCenter\u8fdb\u884c\u96c6\u6210\uff0c\u53ef\u4ee5\u6b63\u5e38\u901a\u8fc7ACI\u8fdb\u884cVM\u7f51\u7edc\u7684\u90e8\u7f72\u3002 2. \u53ef\u4ee5\u6b63\u5e38\u8bbf\u95eeInternet\u7684Common L3out \u5728\u672c\u6f14\u793a\u4e2d\uff0c\u901a\u8fc7common VRF\u4e2d\u7684L3out\u5b9e\u73b0\u5bf9Internet\u7684\u8bbf\u95ee\uff0c\u5b8c\u6210K8s\u7684\u5b89\u88c5 3. HX FI uplink\u6dfb\u52a0\u76f8\u5173VLAN \u521b\u5efakubeapi_vlan, service_vlan \u5728vm-networK-a vNIC template\u4e2d\u6dfb\u52a0\u8fd9\u4e24\u4e2aVLAN 4. SNAT to ACI \u5728L3out router\u4e0a\uff0c\u914d\u7f6eSNAT\u7528\u4e8e\u8bbf\u95eeK8s external service","title":"\u96c6\u6210\u524d\u51c6\u5907"},{"location":"k8s/k8s-prerequisite/#_1","text":"Prerequisite before Integration K8s\u652f\u6301\u8fd0\u884c\u4e0eBareMetal\u670d\u52a1\u5668\u548cVM on Hypervisor\uff0c\u5728\u672c\u6f14\u793a\u4e2d\uff0c\u91c7\u7528K8s on VM\u7684\u90e8\u7f72\u65b9\u5f0f\u3002","title":"\u96c6\u6210\u524d\u51c6\u5907"},{"location":"k8s/k8s-prerequisite/#k8s","text":"NTP: 10.66.141.50, 10.66.141.51 DNS: 64.104.123.245 Node OOB IP: Master: 10.75.53.81 Worker1: 10.75.53.82 Worker2: 10.75.53.83 kubeapi_vlan: 4001 (\u7528\u4e8enode\u7684physical domain) service_vlan: 4003 \uff08\u7528\u4e8eservice\u7684loadbalancer\uff09 Infra_vlan: 4093 node_subnet: 172.16.0.1/24 pod_subnet: 172.16.1.1/24 extern_dynamic: 172.16.2.1/24 extern_static: 172.16.3.1/24 node_svc_subnet: 172.16.4.1/24 cluster_svc_subnet: 172.16.5.1/24 mcast_fabric: 225.1.1.1 mcast_range: start: 225.100.1.1 end: 225.100.1.255","title":"K8s\u7f51\u7edc\u5730\u5740\u89c4\u5212"},{"location":"k8s/k8s-prerequisite/#aci","text":"ACI prerequisite","title":"ACI \u73af\u5883\u51c6\u5907"},{"location":"k8s/k8s-prerequisite/#1-aci-vmmacivcenteracivm","text":"","title":"1. ACI VMM\u914d\u7f6e\uff0cACI\u5e94\u8be5\u6b63\u5e38\u548cVCenter\u8fdb\u884c\u96c6\u6210\uff0c\u53ef\u4ee5\u6b63\u5e38\u901a\u8fc7ACI\u8fdb\u884cVM\u7f51\u7edc\u7684\u90e8\u7f72\u3002"},{"location":"k8s/k8s-prerequisite/#2-internetcommon-l3out","text":"\u5728\u672c\u6f14\u793a\u4e2d\uff0c\u901a\u8fc7common VRF\u4e2d\u7684L3out\u5b9e\u73b0\u5bf9Internet\u7684\u8bbf\u95ee\uff0c\u5b8c\u6210K8s\u7684\u5b89\u88c5","title":"2. \u53ef\u4ee5\u6b63\u5e38\u8bbf\u95eeInternet\u7684Common L3out"},{"location":"k8s/k8s-prerequisite/#3-hx-fi-uplinkvlan","text":"\u521b\u5efakubeapi_vlan, service_vlan \u5728vm-networK-a vNIC template\u4e2d\u6dfb\u52a0\u8fd9\u4e24\u4e2aVLAN","title":"3. HX FI uplink\u6dfb\u52a0\u76f8\u5173VLAN"},{"location":"k8s/k8s-prerequisite/#4-snat-to-aci","text":"\u5728L3out router\u4e0a\uff0c\u914d\u7f6eSNAT\u7528\u4e8e\u8bbf\u95eeK8s external service","title":"4. SNAT to ACI"},{"location":"k8s/k8s-troubleshooting/","text":"K8s\u6392\u9519 kubectl cheatsheet Uninstall k8s on nodes kubeadm reset Restart kubelet service systemctl restart kubelet \u5feb\u901f\u521b\u5efa\u4e00\u4e2assh client\u5bb9\u5668\uff0c\u53ef\u4ee5\u8fdb\u884cping\u548cssh\u6d4b\u8bd5 \uff08\u52a0 -n \u53ef\u4ee5\u6307\u5b9anamespace\uff0c\u5426\u5219\u4e3a\u7f3a\u7701\u7684kebe-default\uff09 kubectl run ssh-client --image=kroniak/ssh-client --restart=Never --rm -it /bin/sh \u5378\u8f7dCNI Plugin kubectl delete -f aci-containers.yaml \u4fee\u6539annotate kubectl edit namespace guestbook \u91cd\u65b0\u5f97\u5230kubeadm join token kubeadm token create --print-join-command","title":"K8s\u6392\u9519"},{"location":"k8s/k8s-troubleshooting/#k8s","text":"kubectl cheatsheet Uninstall k8s on nodes kubeadm reset Restart kubelet service systemctl restart kubelet \u5feb\u901f\u521b\u5efa\u4e00\u4e2assh client\u5bb9\u5668\uff0c\u53ef\u4ee5\u8fdb\u884cping\u548cssh\u6d4b\u8bd5 \uff08\u52a0 -n \u53ef\u4ee5\u6307\u5b9anamespace\uff0c\u5426\u5219\u4e3a\u7f3a\u7701\u7684kebe-default\uff09 kubectl run ssh-client --image=kroniak/ssh-client --restart=Never --rm -it /bin/sh \u5378\u8f7dCNI Plugin kubectl delete -f aci-containers.yaml \u4fee\u6539annotate kubectl edit namespace guestbook \u91cd\u65b0\u5f97\u5230kubeadm join token kubeadm token create --print-join-command","title":"K8s\u6392\u9519"},{"location":"rhv/rhv-index/","text":"ACI Red Hat VMM\u96c6\u6210 \u80cc\u666f\u4fe1\u606f \u672c\u6587\u5c06\u6f14\u793a\u5982\u4f55\u5b9e\u73b0ACI\u4e0eRed Hat Virutalization (RHV)\u7684\u96c6\u6210\u3002 \u7ea2\u5e3d\u00ae\u865a\u62df\u5316\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u8f6f\u4ef6\u5b9a\u4e49\u5e73\u53f0\uff0c\u53ef\u4ee5\u865a\u62df\u5316 Linux \u548c Microsoft Windows \u5de5\u4f5c\u8d1f\u8f7d\u3002\u5b83\u4ee5\u7ea2\u5e3d\u4f01\u4e1a Linux\u00ae \u548c\u57fa\u4e8e\u5185\u6838\u7684\u865a\u62df\u673a\uff08KVM\uff09\u4e3a\u9aa8\u67b6\uff0c\u5185\u542b\u7684\u7ba1\u7406\u5de5\u5177\u53ef\u4ee5\u865a\u62df\u5316\u8d44\u6e90\u3001\u8fdb\u7a0b\u548c\u5e94\u7528\u7a0b\u5e8f\uff0c\u4ece\u800c\u4e3a\u672a\u6765\u7684\u4e91\u539f\u751f\u548c\u5bb9\u5668\u5316\u63d0\u4f9b\u4e00\u4e2a\u7a33\u5b9a\u7684\u57fa\u7840\u3002\u66f4\u591a\u5173\u4e8eRHV\u7684\u4fe1\u606f\u8bf7\u6d4f\u89c8\u7ea2\u5e3d\u5b98\u65b9\u7f51\u7ad9\uff1a \u7ea2\u5e3d\u865a\u62df\u5316\u4e2d\u6587\u5b98\u7f51 RHV\u5b89\u88c5\u6587\u6863\uff1a Red Hat Virtualization 4.2 installation guide \u601d\u79d1\u76f8\u5173\u6587\u6863 \u601d\u79d1ACI\u4e0eRHV\u96c6\u6210\u7248\u672cMatrix \u601d\u79d1ACI\u4e0eRHV\u96c6\u6210\u6587\u6863","title":"\u80cc\u666f\u4fe1\u606f"},{"location":"rhv/rhv-index/#aci-red-hat-vmm","text":"","title":"ACI Red Hat VMM\u96c6\u6210"},{"location":"rhv/rhv-index/#_1","text":"\u672c\u6587\u5c06\u6f14\u793a\u5982\u4f55\u5b9e\u73b0ACI\u4e0eRed Hat Virutalization (RHV)\u7684\u96c6\u6210\u3002 \u7ea2\u5e3d\u00ae\u865a\u62df\u5316\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u8f6f\u4ef6\u5b9a\u4e49\u5e73\u53f0\uff0c\u53ef\u4ee5\u865a\u62df\u5316 Linux \u548c Microsoft Windows \u5de5\u4f5c\u8d1f\u8f7d\u3002\u5b83\u4ee5\u7ea2\u5e3d\u4f01\u4e1a Linux\u00ae \u548c\u57fa\u4e8e\u5185\u6838\u7684\u865a\u62df\u673a\uff08KVM\uff09\u4e3a\u9aa8\u67b6\uff0c\u5185\u542b\u7684\u7ba1\u7406\u5de5\u5177\u53ef\u4ee5\u865a\u62df\u5316\u8d44\u6e90\u3001\u8fdb\u7a0b\u548c\u5e94\u7528\u7a0b\u5e8f\uff0c\u4ece\u800c\u4e3a\u672a\u6765\u7684\u4e91\u539f\u751f\u548c\u5bb9\u5668\u5316\u63d0\u4f9b\u4e00\u4e2a\u7a33\u5b9a\u7684\u57fa\u7840\u3002\u66f4\u591a\u5173\u4e8eRHV\u7684\u4fe1\u606f\u8bf7\u6d4f\u89c8\u7ea2\u5e3d\u5b98\u65b9\u7f51\u7ad9\uff1a \u7ea2\u5e3d\u865a\u62df\u5316\u4e2d\u6587\u5b98\u7f51","title":"\u80cc\u666f\u4fe1\u606f"},{"location":"rhv/rhv-index/#rhv","text":"Red Hat Virtualization 4.2 installation guide","title":"RHV\u5b89\u88c5\u6587\u6863\uff1a"},{"location":"rhv/rhv-index/#_2","text":"\u601d\u79d1ACI\u4e0eRHV\u96c6\u6210\u7248\u672cMatrix \u601d\u79d1ACI\u4e0eRHV\u96c6\u6210\u6587\u6863","title":"\u601d\u79d1\u76f8\u5173\u6587\u6863"},{"location":"rhv/rhv-topo/","text":"ACI Red Hat VMM\u96c6\u6210 \u80cc\u666f\u4fe1\u606f \u8f6f\u4ef6\u7248\u672c\uff1a ACI: 4.2(3J) RHEL: 7.2 RHV: 4.2 \u5b9e\u9a8c\u62d3\u6251 IP\u5730\u5740\u4fe1\u606f\uff1a RHVM: 10.75.53.134 RHV Host: 10.75.53.135 DNS Server: 10.75.53.59 \u57fa\u672c\u6b65\u9aa4 \u5b89\u88c5Red Hat Virtualization Manager \u5b89\u88c5Red Hat Virtualization Host \u5728RHVM\u4e2d\u521b\u5efa\u865a\u62df\u673a ACI\u4e0eRHV\u96c6\u6210 \u5b89\u88c5RHV \u5728vCenter\u4e2d\u5b89\u88c5RHEL","title":"\u5b9e\u9a8c\u62d3\u6251"},{"location":"rhv/rhv-topo/#aci-red-hat-vmm","text":"","title":"ACI Red Hat VMM\u96c6\u6210"},{"location":"rhv/rhv-topo/#_1","text":"\u8f6f\u4ef6\u7248\u672c\uff1a ACI: 4.2(3J) RHEL: 7.2 RHV: 4.2","title":"\u80cc\u666f\u4fe1\u606f"},{"location":"rhv/rhv-topo/#_2","text":"IP\u5730\u5740\u4fe1\u606f\uff1a RHVM: 10.75.53.134 RHV Host: 10.75.53.135 DNS Server: 10.75.53.59","title":"\u5b9e\u9a8c\u62d3\u6251"},{"location":"rhv/rhv-topo/#_3","text":"\u5b89\u88c5Red Hat Virtualization Manager \u5b89\u88c5Red Hat Virtualization Host \u5728RHVM\u4e2d\u521b\u5efa\u865a\u62df\u673a ACI\u4e0eRHV\u96c6\u6210","title":"\u57fa\u672c\u6b65\u9aa4"},{"location":"rhv/rhv-topo/#rhv","text":"\u5728vCenter\u4e2d\u5b89\u88c5RHEL","title":"\u5b89\u88c5RHV"},{"location":"ztp/ztp-index/","text":"ACI\u81ea\u52a8\u5316\u4e0a\u7ebf \u80cc\u666f\u77e5\u8bc6 \u672c\u6587\u5c06\u6f14\u793a\u5982\u4f55\u5b8c\u6210ACI\u9996\u6b21\u4e0a\u7ebf\u3002 ACI\u4e0a\u7ebf\u7684\u57fa\u672c\u6b65\u9aa4\u5982\u4e0b\uff1a \u7b2c\u4e00\u53f0APIC\u5f00\u673a \uff08CIMC, ACI initial setup script\uff09 \u53d1\u73b0\u5e76\u6ce8\u518c\u7b2c\u4e00\u53f0Leaf \u53d1\u73b0\u5e76\u6ce8\u518c\u7b2c\u4e00\u53f0Spine \u53d1\u73b0\u5e76\u6ce8\u518c\u5269\u4f59\u7684Leaf\u548cSpine \u6ce8\u518c\u5269\u4f59\u7684APIC \u81ea\u52a8\u5f00\u5c40\u5b8c\u6210 \u5b8c\u6210\u5176\u4ed6\u521d\u59cb\u5316\u914d\u7f6e\uff0c\u5982OOB, NTP\u7b49","title":"\u80cc\u666f\u4fe1\u606f"},{"location":"ztp/ztp-index/#aci","text":"","title":"ACI\u81ea\u52a8\u5316\u4e0a\u7ebf"},{"location":"ztp/ztp-index/#_1","text":"\u672c\u6587\u5c06\u6f14\u793a\u5982\u4f55\u5b8c\u6210ACI\u9996\u6b21\u4e0a\u7ebf\u3002 ACI\u4e0a\u7ebf\u7684\u57fa\u672c\u6b65\u9aa4\u5982\u4e0b\uff1a \u7b2c\u4e00\u53f0APIC\u5f00\u673a \uff08CIMC, ACI initial setup script\uff09 \u53d1\u73b0\u5e76\u6ce8\u518c\u7b2c\u4e00\u53f0Leaf \u53d1\u73b0\u5e76\u6ce8\u518c\u7b2c\u4e00\u53f0Spine \u53d1\u73b0\u5e76\u6ce8\u518c\u5269\u4f59\u7684Leaf\u548cSpine \u6ce8\u518c\u5269\u4f59\u7684APIC \u81ea\u52a8\u5f00\u5c40\u5b8c\u6210 \u5b8c\u6210\u5176\u4ed6\u521d\u59cb\u5316\u914d\u7f6e\uff0c\u5982OOB, NTP\u7b49","title":"\u80cc\u666f\u77e5\u8bc6"},{"location":"ztp/ztp-note/","text":"PoV Lab\u73af\u5883\u64cd\u4f5c\u89c4\u5219 PoV Lab\u4e3a\u591a\u4eba\u4f7f\u7528\u73af\u5883\uff0c\u4e3a\u4e86\u4fdd\u8bc1\u5927\u5bb6Lab\u8fc7\u7a0b\u987a\u5229\u548cPoV Lab\u7684\u53ef\u6301\u7eed\u6027\uff0c\u8bf7\u5728\u4f7f\u7528\u671f\u95f4\u9075\u5faa\u4ee5\u4e0b\u89c4\u5219\uff0c\u8c22\u8c22\uff01 \u4f7f\u7528\u5206\u914d\u7684\u8d26\u53f7\uff08CEC ID\uff09\u767b\u5f55\u5b9e\u9a8c\u8bbe\u5907 \u5728APIC\u4e2d\u521b\u5efa\u81ea\u5df1\u7684Tenant\uff0c\u547d\u540d\u89c4\u5219\u4e3a CEC_ID-Name \uff0c\u5982mentan-K8S\u3002\u6240\u6709\u7684\u914d\u7f6e\u5e94\u5728\u81ea\u5df1\u7684Tenant\u5185\u8fdb\u884c\uff0c\u5982\u65e0\u7279\u6b8a\u9700\u8981\u8bf7\u4e0d\u8981\u4fee\u6539\u7cfb\u7edf\u7f3a\u7701Tenant\u7684\u4efb\u4f55\u914d\u7f6e\uff0c\u5982\u6709\u7279\u6b8a\u9700\u6c42\u8bf7\u8054\u7cfbTME Lab\u7ba1\u7406\u4eba\u3002 \u6240\u6709\u7684policy\uff0c\u5982access-policy\uff0cl3out\u7b49\uff0c\u90fd\u5e94\u5f53\u9075\u5faa CEC_ID-Name \u7684\u547d\u540d\u89c4\u5219\uff0c\u5982mentan-vlanpool1, mentan-vpc1, mentan-l3out1\u7b49\u3002 \u4ee5 POV-Name \u5f00\u5934\u7684\u7b56\u7565\u4e3a\u57fa\u7840\u67b6\u6784\u7b56\u7565\uff0c\u8bf7\u52ff\u968f\u610f\u5220\u9664\u548c\u4fee\u6539 \u5bf9\u4e8e\u6ca1\u6709\u9075\u5faa\u547d\u540d\u89c4\u5219\u7684\u914d\u7f6e\u5c06\u4f1a\u5728\u65e0\u901a\u77e5\u7684\u60c5\u51b5\u4e0b\u5b9a\u671f\u6e05\u9664\uff0c\u8bf7\u77e5\u6089\u3002 \u8bf7\u4e0d\u8981\u968f\u610f\u5bf9Fabric\u8fdb\u884c\u5347\u7ea7/\u964d\u7ea7\u64cd\u4f5c\uff0c\u5982\u6709\u9700\u8981\u8bf7\u8054\u7cfbTME Lab\u7ba1\u7406\u4eba \u672c\u5b9e\u9a8c\u5c06\u5c55\u793a\u4ee5\u4e0b\u7279\u6027\uff1a ACI Zero touch\u4e0a\u7ebf \uff08*ACI Simulator\uff09 APIC\u5b8c\u6210Underlay ISIS\u7f51\u7edc\u7684\u521d\u59cb\u5316 \u7f51\u7edc\u81ea\u52a8\u53d1\u73b0\uff08LLDP exchange\uff09 IP\u5730\u5740\u81ea\u52a8\u5206\u914d ISIS\u90bb\u5c45\u5efa\u7acb APIC\u5b8c\u6210Overlay VXLAN\u7f51\u7edc\u7684\u521d\u59cb\u5316 \u4ea4\u6362\u673a\u5206\u914dVTEP\u5730\u5740 \uff08\u81ea\u52a8\uff09 \u8bbe\u7f6eOverlay\u8def\u7531\u534f\u8bae \uff08MP-BPG\uff09 *\u4e3a\u4e86\u907f\u514d\u521d\u59cb\u5316ACI fabric\u5bf9PoV lab\u7684\u5f71\u54cd\uff0cZero touch\u4e0a\u7ebf\u6f14\u793a\u5c06\u901a\u8fc7ACI Simulator\u6a21\u62df\u3002","title":"\u6ce8\u610f\u4e8b\u9879"},{"location":"ztp/ztp-note/#pov-lab","text":"","title":"PoV Lab\u73af\u5883\u64cd\u4f5c\u89c4\u5219"},{"location":"ztp/ztp-note/#pov-lablabpov-lab","text":"\u4f7f\u7528\u5206\u914d\u7684\u8d26\u53f7\uff08CEC ID\uff09\u767b\u5f55\u5b9e\u9a8c\u8bbe\u5907 \u5728APIC\u4e2d\u521b\u5efa\u81ea\u5df1\u7684Tenant\uff0c\u547d\u540d\u89c4\u5219\u4e3a CEC_ID-Name \uff0c\u5982mentan-K8S\u3002\u6240\u6709\u7684\u914d\u7f6e\u5e94\u5728\u81ea\u5df1\u7684Tenant\u5185\u8fdb\u884c\uff0c\u5982\u65e0\u7279\u6b8a\u9700\u8981\u8bf7\u4e0d\u8981\u4fee\u6539\u7cfb\u7edf\u7f3a\u7701Tenant\u7684\u4efb\u4f55\u914d\u7f6e\uff0c\u5982\u6709\u7279\u6b8a\u9700\u6c42\u8bf7\u8054\u7cfbTME Lab\u7ba1\u7406\u4eba\u3002 \u6240\u6709\u7684policy\uff0c\u5982access-policy\uff0cl3out\u7b49\uff0c\u90fd\u5e94\u5f53\u9075\u5faa CEC_ID-Name \u7684\u547d\u540d\u89c4\u5219\uff0c\u5982mentan-vlanpool1, mentan-vpc1, mentan-l3out1\u7b49\u3002 \u4ee5 POV-Name \u5f00\u5934\u7684\u7b56\u7565\u4e3a\u57fa\u7840\u67b6\u6784\u7b56\u7565\uff0c\u8bf7\u52ff\u968f\u610f\u5220\u9664\u548c\u4fee\u6539 \u5bf9\u4e8e\u6ca1\u6709\u9075\u5faa\u547d\u540d\u89c4\u5219\u7684\u914d\u7f6e\u5c06\u4f1a\u5728\u65e0\u901a\u77e5\u7684\u60c5\u51b5\u4e0b\u5b9a\u671f\u6e05\u9664\uff0c\u8bf7\u77e5\u6089\u3002 \u8bf7\u4e0d\u8981\u968f\u610f\u5bf9Fabric\u8fdb\u884c\u5347\u7ea7/\u964d\u7ea7\u64cd\u4f5c\uff0c\u5982\u6709\u9700\u8981\u8bf7\u8054\u7cfbTME Lab\u7ba1\u7406\u4eba","title":"PoV Lab\u4e3a\u591a\u4eba\u4f7f\u7528\u73af\u5883\uff0c\u4e3a\u4e86\u4fdd\u8bc1\u5927\u5bb6Lab\u8fc7\u7a0b\u987a\u5229\u548cPoV Lab\u7684\u53ef\u6301\u7eed\u6027\uff0c\u8bf7\u5728\u4f7f\u7528\u671f\u95f4\u9075\u5faa\u4ee5\u4e0b\u89c4\u5219\uff0c\u8c22\u8c22\uff01"},{"location":"ztp/ztp-note/#_1","text":"ACI Zero touch\u4e0a\u7ebf \uff08*ACI Simulator\uff09 APIC\u5b8c\u6210Underlay ISIS\u7f51\u7edc\u7684\u521d\u59cb\u5316 \u7f51\u7edc\u81ea\u52a8\u53d1\u73b0\uff08LLDP exchange\uff09 IP\u5730\u5740\u81ea\u52a8\u5206\u914d ISIS\u90bb\u5c45\u5efa\u7acb APIC\u5b8c\u6210Overlay VXLAN\u7f51\u7edc\u7684\u521d\u59cb\u5316 \u4ea4\u6362\u673a\u5206\u914dVTEP\u5730\u5740 \uff08\u81ea\u52a8\uff09 \u8bbe\u7f6eOverlay\u8def\u7531\u534f\u8bae \uff08MP-BPG\uff09 *\u4e3a\u4e86\u907f\u514d\u521d\u59cb\u5316ACI fabric\u5bf9PoV lab\u7684\u5f71\u54cd\uff0cZero touch\u4e0a\u7ebf\u6f14\u793a\u5c06\u901a\u8fc7ACI Simulator\u6a21\u62df\u3002","title":"\u672c\u5b9e\u9a8c\u5c06\u5c55\u793a\u4ee5\u4e0b\u7279\u6027\uff1a"},{"location":"ztp/ztp-scenario/","text":"\u6f14\u793a\u573a\u666f1 ACI Zero Touch Provision\u4e0a\u7ebf ACI Simulator\u662f\u4e00\u4e2a\u57fa\u4e8eVM\u7684ACI\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u6a21\u62dfACI\u63a7\u5236\u5e73\u9762\u7684\u7528\u6237\u754c\u9762\u548c\u914d\u7f6e\u8fc7\u7a0b\u3002\u57fa\u672c\u7684\u6a21\u62df\u62d3\u6251\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5728\u672c\u6f14\u793a\u4e2d\uff0c\u6211\u4eec\u5c06\u5229\u7528Simulator\u6f14\u793aACI\u4e0a\u7ebf\u7684\u8fc7\u7a0b\u3002 \u6ce8\u610f\uff1aACI Simulator\u53ea\u80fd\u5c55\u793a\u63a7\u5236\u5e73\u9762\u7684\u914d\u7f6e\u8fc7\u7a0b\uff0c\u65e0\u6cd5\u6a21\u62df\u6570\u636e\u8f6c\u53d1\u5e73\u9762\uff0c\u4e5f\u5c31\u662f\u8bf4\u5728Simulator\u4e2d\u65e0\u6cd5\u4ea7\u751f\u771f\u6b63\u7684\u6570\u636e\u6d41\u3002","title":"\u573a\u666f\u6f14\u793a"},{"location":"ztp/ztp-scenario/#1-aci-zero-touch-provision","text":"ACI Simulator\u662f\u4e00\u4e2a\u57fa\u4e8eVM\u7684ACI\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u6a21\u62dfACI\u63a7\u5236\u5e73\u9762\u7684\u7528\u6237\u754c\u9762\u548c\u914d\u7f6e\u8fc7\u7a0b\u3002\u57fa\u672c\u7684\u6a21\u62df\u62d3\u6251\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5728\u672c\u6f14\u793a\u4e2d\uff0c\u6211\u4eec\u5c06\u5229\u7528Simulator\u6f14\u793aACI\u4e0a\u7ebf\u7684\u8fc7\u7a0b\u3002 \u6ce8\u610f\uff1aACI Simulator\u53ea\u80fd\u5c55\u793a\u63a7\u5236\u5e73\u9762\u7684\u914d\u7f6e\u8fc7\u7a0b\uff0c\u65e0\u6cd5\u6a21\u62df\u6570\u636e\u8f6c\u53d1\u5e73\u9762\uff0c\u4e5f\u5c31\u662f\u8bf4\u5728Simulator\u4e2d\u65e0\u6cd5\u4ea7\u751f\u771f\u6b63\u7684\u6570\u636e\u6d41\u3002","title":"\u6f14\u793a\u573a\u666f1 ACI Zero Touch Provision\u4e0a\u7ebf"},{"location":"ztp/ztp-topo/","text":"\u5b9e\u9a8c\u62d3\u6251 ACI Simulator\u662f\u4e00\u4e2a\u57fa\u4e8eVM\u7684ACI\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u6a21\u62dfACI\u63a7\u5236\u5e73\u9762\u7684\u7528\u6237\u754c\u9762\u548c\u914d\u7f6e\u8fc7\u7a0b\u3002\u57fa\u672c\u7684\u6a21\u62df\u62d3\u6251\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5728\u672c\u6f14\u793a\u4e2d\uff0c\u6211\u4eec\u5c06\u5229\u7528Simulator\u6f14\u793aACI\u4e0a\u7ebf\u7684\u8fc7\u7a0b\u3002 \u6ce8\u610f\uff1aACI Simulator\u53ea\u80fd\u5c55\u793a\u63a7\u5236\u5e73\u9762\u7684\u914d\u7f6e\u8fc7\u7a0b\uff0c\u65e0\u6cd5\u6a21\u62df\u6570\u636e\u8f6c\u53d1\u5e73\u9762\uff0c\u4e5f\u5c31\u662f\u8bf4\u5728Simulator\u4e2d\u65e0\u6cd5\u4ea7\u751f\u771f\u6b63\u7684\u6570\u636e\u6d41\u3002 \u5b9e\u9a8c\u8bbf\u95ee\u4fe1\u606f Pod# Attender IP Address Username Password 1 2 3 4 5 6","title":"\u5b9e\u9a8c\u62d3\u6251"},{"location":"ztp/ztp-topo/#_1","text":"ACI Simulator\u662f\u4e00\u4e2a\u57fa\u4e8eVM\u7684ACI\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u6a21\u62dfACI\u63a7\u5236\u5e73\u9762\u7684\u7528\u6237\u754c\u9762\u548c\u914d\u7f6e\u8fc7\u7a0b\u3002\u57fa\u672c\u7684\u6a21\u62df\u62d3\u6251\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5728\u672c\u6f14\u793a\u4e2d\uff0c\u6211\u4eec\u5c06\u5229\u7528Simulator\u6f14\u793aACI\u4e0a\u7ebf\u7684\u8fc7\u7a0b\u3002 \u6ce8\u610f\uff1aACI Simulator\u53ea\u80fd\u5c55\u793a\u63a7\u5236\u5e73\u9762\u7684\u914d\u7f6e\u8fc7\u7a0b\uff0c\u65e0\u6cd5\u6a21\u62df\u6570\u636e\u8f6c\u53d1\u5e73\u9762\uff0c\u4e5f\u5c31\u662f\u8bf4\u5728Simulator\u4e2d\u65e0\u6cd5\u4ea7\u751f\u771f\u6b63\u7684\u6570\u636e\u6d41\u3002","title":"\u5b9e\u9a8c\u62d3\u6251"},{"location":"ztp/ztp-topo/#_2","text":"Pod# Attender IP Address Username Password 1 2 3 4 5 6","title":"\u5b9e\u9a8c\u8bbf\u95ee\u4fe1\u606f"}]}